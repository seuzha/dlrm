{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# miscellaneous\n",
    "import builtins\n",
    "import functools\n",
    "# import bisect\n",
    "# import shutil\n",
    "import time\n",
    "import json\n",
    "# data generation\n",
    "import dlrm_data_pytorch as dp\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# onnx\n",
    "# The onnx import causes deprecation warnings every time workers\n",
    "# are spawned during testing. So, we filter out those warnings.\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import onnx\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel.parallel_apply import parallel_apply\n",
    "from torch.nn.parallel.replicate import replicate\n",
    "from torch.nn.parallel.scatter_gather import gather, scatter\n",
    "# quotient-remainder trick\n",
    "from tricks.qr_embedding_bag import QREmbeddingBag\n",
    "# mixed-dimension trick\n",
    "from tricks.md_embedding_bag import PrEmbeddingBag, md_solver\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "# from torchviz import make_dot\n",
    "# import torch.nn.functional as Functional\n",
    "# from torch.nn.parameter import Parameter\n",
    "\n",
    "exc = getattr(builtins, \"IOError\", \"FileNotFoundError\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import packages ###\n",
    "import sys\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define dlrm in PyTorch ###\n",
    "class DLRM_Net(nn.Module):\n",
    "    def create_mlp(self, ln, sigmoid_layer):\n",
    "        # build MLP layer by layer\n",
    "        layers = nn.ModuleList()\n",
    "        for i in range(0, ln.size - 1):\n",
    "            n = ln[i]\n",
    "            m = ln[i + 1]\n",
    "\n",
    "            # construct fully connected operator\n",
    "            LL = nn.Linear(int(n), int(m), bias=True)\n",
    "\n",
    "            # initialize the weights\n",
    "            # with torch.no_grad():\n",
    "            # custom Xavier input, output or two-sided fill\n",
    "            mean = 0.0  # std_dev = np.sqrt(variance)\n",
    "            std_dev = np.sqrt(2 / (m + n))  # np.sqrt(1 / m) # np.sqrt(1 / n)\n",
    "            W = np.random.normal(mean, std_dev, size=(m, n)).astype(np.float32)\n",
    "            std_dev = np.sqrt(1 / m)  # np.sqrt(2 / (m + 1))\n",
    "            bt = np.random.normal(mean, std_dev, size=m).astype(np.float32)\n",
    "            # approach 1\n",
    "            LL.weight.data = torch.tensor(W, requires_grad=True)\n",
    "            LL.bias.data = torch.tensor(bt, requires_grad=True)\n",
    "            # approach 2\n",
    "            # LL.weight.data.copy_(torch.tensor(W))\n",
    "            # LL.bias.data.copy_(torch.tensor(bt))\n",
    "            # approach 3\n",
    "            # LL.weight = Parameter(torch.tensor(W),requires_grad=True)\n",
    "            # LL.bias = Parameter(torch.tensor(bt),requires_grad=True)\n",
    "            layers.append(LL)\n",
    "\n",
    "            # construct sigmoid or relu operator\n",
    "            if i == sigmoid_layer:\n",
    "                layers.append(nn.Sigmoid())\n",
    "            else:\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "        # approach 1: use ModuleList\n",
    "        # return layers\n",
    "        # approach 2: use Sequential container to wrap all layers\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def create_emb(self, m, ln):\n",
    "        # litez: m is embedding_dim;\n",
    "        # ln gives a list of vocabulary size for each \n",
    "        # of the sparse feature\n",
    "        emb_l = nn.ModuleList()\n",
    "        for i in range(0, ln.size):\n",
    "            n = ln[i]\n",
    "            # construct embedding operator\n",
    "            if self.qr_flag and n > self.qr_threshold:\n",
    "                EE = QREmbeddingBag(n, m, self.qr_collisions,\n",
    "                    operation=self.qr_operation, mode=\"sum\", sparse=True)\n",
    "            elif self.md_flag and n > self.md_threshold:\n",
    "                _m = m[i]\n",
    "                base = max(m)\n",
    "                EE = PrEmbeddingBag(n, _m, base)\n",
    "                # use np initialization as below for consistency...\n",
    "                W = np.random.uniform(\n",
    "                    low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, _m)\n",
    "                ).astype(np.float32)\n",
    "                EE.embs.weight.data = torch.tensor(W, requires_grad=True)\n",
    "\n",
    "            else:\n",
    "                EE = nn.EmbeddingBag(n, m, mode=\"sum\", sparse=True)\n",
    "\n",
    "                # initialize embeddings\n",
    "                # nn.init.uniform_(EE.weight, a=-np.sqrt(1 / n), b=np.sqrt(1 / n))\n",
    "                W = np.random.uniform(\n",
    "                    low=-np.sqrt(1 / n), high=np.sqrt(1 / n), size=(n, m)\n",
    "                ).astype(np.float32)\n",
    "                # approach 1\n",
    "                EE.weight.data = torch.tensor(W, requires_grad=True)\n",
    "                # approach 2\n",
    "                # EE.weight.data.copy_(torch.tensor(W))\n",
    "                # approach 3\n",
    "                # EE.weight = Parameter(torch.tensor(W),requires_grad=True)\n",
    "\n",
    "            emb_l.append(EE)\n",
    "\n",
    "        return emb_l\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        m_spa=None,\n",
    "        ln_emb=None,\n",
    "        ln_bot=None,\n",
    "        ln_top=None,\n",
    "        arch_interaction_op=None,\n",
    "        arch_interaction_itself=False,\n",
    "        sigmoid_bot=-1,\n",
    "        sigmoid_top=-1,\n",
    "        sync_dense_params=True,\n",
    "        loss_threshold=0.0,\n",
    "        ndevices=-1,\n",
    "        qr_flag=False,\n",
    "        qr_operation=\"mult\",\n",
    "        qr_collisions=0,\n",
    "        qr_threshold=200,\n",
    "        md_flag=False,\n",
    "        md_threshold=200,\n",
    "    ):\n",
    "        super(DLRM_Net, self).__init__()\n",
    "\n",
    "        if (\n",
    "            (m_spa is not None)\n",
    "            and (ln_emb is not None)\n",
    "            and (ln_bot is not None)\n",
    "            and (ln_top is not None)\n",
    "            and (arch_interaction_op is not None)\n",
    "        ):\n",
    "\n",
    "            # save arguments\n",
    "            self.ndevices = ndevices\n",
    "            self.output_d = 0\n",
    "            self.parallel_model_batch_size = -1\n",
    "            self.parallel_model_is_not_prepared = True\n",
    "            self.arch_interaction_op = arch_interaction_op\n",
    "            self.arch_interaction_itself = arch_interaction_itself\n",
    "            self.sync_dense_params = sync_dense_params\n",
    "            self.loss_threshold = loss_threshold\n",
    "            # create variables for QR embedding if applicable\n",
    "            self.qr_flag = qr_flag\n",
    "            if self.qr_flag:\n",
    "                self.qr_collisions = qr_collisions\n",
    "                self.qr_operation = qr_operation\n",
    "                self.qr_threshold = qr_threshold\n",
    "            # create variables for MD embedding if applicable\n",
    "            self.md_flag = md_flag\n",
    "            if self.md_flag:\n",
    "                self.md_threshold = md_threshold\n",
    "            # create operators\n",
    "            if ndevices <= 1:\n",
    "                self.emb_l = self.create_emb(m_spa, ln_emb)\n",
    "            self.bot_l = self.create_mlp(ln_bot, sigmoid_bot)\n",
    "            self.top_l = self.create_mlp(ln_top, sigmoid_top)\n",
    "\n",
    "    def apply_mlp(self, x, layers):\n",
    "        # approach 1: use ModuleList\n",
    "        # for layer in layers:\n",
    "        #     x = layer(x)\n",
    "        # return x\n",
    "        # approach 2: use Sequential container to wrap all layers\n",
    "        return layers(x)\n",
    "\n",
    "    def apply_emb(self, lS_o, lS_i, emb_l):\n",
    "        # WARNING: notice that we are processing the batch at once. We implicitly\n",
    "        # assume that the data is laid out such that:\n",
    "        # 1. each embedding is indexed with a group of sparse indices,\n",
    "        #   corresponding to a single lookup\n",
    "        # 2. for each embedding the lookups are further organized into a batch\n",
    "        # 3. for a list of embedding tables there is a list of batched lookups\n",
    "\n",
    "        ly = []\n",
    "        #litez: lS_i is a list of tensors, each of which is of size (Batch_size,1)\\\n",
    "        #Since num_of_sparse elements is 26, we have len(lS_i)=26 in this case.\n",
    "        for k, sparse_index_group_batch in enumerate(lS_i):\n",
    "            sparse_offset_group_batch = lS_o[k]\n",
    "\n",
    "            # embedding lookup\n",
    "            # We are using EmbeddingBag, which implicitly uses sum operator.\n",
    "            # The embeddings are represented as tall matrices, with sum\n",
    "            # happening vertically across 0 axis, resulting in a row vector\n",
    "            E = emb_l[k]\n",
    "            # litez: V.shape =(batch_size, embedding size);\n",
    "            # what is the advantage of using embedding bag?\n",
    "            V = E(sparse_index_group_batch, sparse_offset_group_batch)\n",
    "\n",
    "            ly.append(V)\n",
    "\n",
    "        # print(ly)\n",
    "        return ly\n",
    "\n",
    "    def interact_features(self, x, ly):\n",
    "        if self.arch_interaction_op == \"dot\":\n",
    "            # concatenate dense and sparse features\n",
    "            (batch_size, d) = x.shape\n",
    "            T = torch.cat([x] + ly, dim=1).view((batch_size, -1, d))\n",
    "            # perform a dot product\n",
    "            Z = torch.bmm(T, torch.transpose(T, 1, 2))\n",
    "            # append dense feature with the interactions (into a row vector)\n",
    "            # approach 1: all\n",
    "            # Zflat = Z.view((batch_size, -1))\n",
    "            # approach 2: unique\n",
    "            _, ni, nj = Z.shape\n",
    "            # approach 1: tril_indices\n",
    "            # offset = 0 if self.arch_interaction_itself else -1\n",
    "            # li, lj = torch.tril_indices(ni, nj, offset=offset)\n",
    "            # approach 2: custom\n",
    "            offset = 1 if self.arch_interaction_itself else 0\n",
    "            li = torch.tensor([i for i in range(ni) for j in range(i + offset)])\n",
    "            lj = torch.tensor([j for i in range(nj) for j in range(i + offset)])\n",
    "            Zflat = Z[:, li, lj]\n",
    "            # concatenate dense features and interactions\n",
    "            R = torch.cat([x] + [Zflat], dim=1)\n",
    "        elif self.arch_interaction_op == \"cat\":\n",
    "            # concatenation features (into a row vector)\n",
    "            R = torch.cat([x] + ly, dim=1)\n",
    "        else:\n",
    "            sys.exit(\n",
    "                \"ERROR: --arch-interaction-op=\"\n",
    "                + self.arch_interaction_op\n",
    "                + \" is not supported\"\n",
    "            )\n",
    "\n",
    "        return R\n",
    "\n",
    "    def forward(self, dense_x, lS_o, lS_i):\n",
    "        if self.ndevices <= 1:\n",
    "            return self.sequential_forward(dense_x, lS_o, lS_i)\n",
    "        else:\n",
    "            return self.parallel_forward(dense_x, lS_o, lS_i)\n",
    "\n",
    "    def sequential_forward(self, dense_x, lS_o, lS_i):\n",
    "        # process dense features (using bottom mlp), resulting in a row vector\n",
    "        x = self.apply_mlp(dense_x, self.bot_l)\n",
    "        # debug prints\n",
    "        # print(\"intermediate\")\n",
    "        # print(x.detach().cpu().numpy())\n",
    "\n",
    "        # process sparse features(using embeddings), resulting in a list of row vectors\n",
    "        # litez: ly is a list of (Batch_size, embedding_size); list lengh equals the number of\n",
    "        # sparese features\n",
    "        ly = self.apply_emb(lS_o, lS_i, self.emb_l)\n",
    "        # for y in ly:\n",
    "        #     print(y.detach().cpu().numpy())\n",
    "\n",
    "        # interact features (dense and sparse)\n",
    "        # litez: z is of shape(batch_size, total_number of features feeding into the top layer mlp)\n",
    "        z = self.interact_features(x, ly)\n",
    "        # print(z.detach().cpu().numpy())\n",
    "\n",
    "        # obtain probability of a click (using top mlp)\n",
    "        p = self.apply_mlp(z, self.top_l)\n",
    "\n",
    "        # clamp output if needed\n",
    "        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:\n",
    "            z = torch.clamp(p, min=self.loss_threshold, max=(1.0 - self.loss_threshold))\n",
    "        else:\n",
    "            z = p\n",
    "\n",
    "        return z\n",
    "\n",
    "    def parallel_forward(self, dense_x, lS_o, lS_i):\n",
    "        ### prepare model (overwrite) ###\n",
    "        # WARNING: # of devices must be >= batch size in parallel_forward call\n",
    "        batch_size = dense_x.size()[0]\n",
    "        ndevices = min(self.ndevices, batch_size, len(self.emb_l))\n",
    "        device_ids = range(ndevices)\n",
    "        # WARNING: must redistribute the model if mini-batch size changes(this is common\n",
    "        # for last mini-batch, when # of elements in the dataset/batch size is not even\n",
    "        if self.parallel_model_batch_size != batch_size:\n",
    "            self.parallel_model_is_not_prepared = True\n",
    "\n",
    "        if self.parallel_model_is_not_prepared or self.sync_dense_params:\n",
    "            # replicate mlp (data parallelism)\n",
    "            self.bot_l_replicas = replicate(self.bot_l, device_ids)\n",
    "            self.top_l_replicas = replicate(self.top_l, device_ids)\n",
    "            self.parallel_model_batch_size = batch_size\n",
    "\n",
    "        if self.parallel_model_is_not_prepared:\n",
    "            # distribute embeddings (model parallelism)\n",
    "            t_list = []\n",
    "            for k, emb in enumerate(self.emb_l):\n",
    "                d = torch.device(\"cuda:\" + str(k % ndevices))\n",
    "                emb.to(d)\n",
    "                t_list.append(emb.to(d))\n",
    "            self.emb_l = nn.ModuleList(t_list)\n",
    "            self.parallel_model_is_not_prepared = False\n",
    "\n",
    "        ### prepare input (overwrite) ###\n",
    "        # scatter dense features (data parallelism)\n",
    "        # print(dense_x.device)\n",
    "        dense_x = scatter(dense_x, device_ids, dim=0)\n",
    "        # distribute sparse features (model parallelism)\n",
    "        if (len(self.emb_l) != len(lS_o)) or (len(self.emb_l) != len(lS_i)):\n",
    "            sys.exit(\"ERROR: corrupted model input detected in parallel_forward call\")\n",
    "\n",
    "        t_list = []\n",
    "        i_list = []\n",
    "        for k, _ in enumerate(self.emb_l):\n",
    "            d = torch.device(\"cuda:\" + str(k % ndevices))\n",
    "            t_list.append(lS_o[k].to(d))\n",
    "            i_list.append(lS_i[k].to(d))\n",
    "        lS_o = t_list\n",
    "        lS_i = i_list\n",
    "\n",
    "        ### compute results in parallel ###\n",
    "        # bottom mlp\n",
    "        # WARNING: Note that the self.bot_l is a list of bottom mlp modules\n",
    "        # that have been replicated across devices, while dense_x is a tuple of dense\n",
    "        # inputs that has been scattered across devices on the first (batch) dimension.\n",
    "        # The output is a list of tensors scattered across devices according to the\n",
    "        # distribution of dense_x.\n",
    "        x = parallel_apply(self.bot_l_replicas, dense_x, None, device_ids)\n",
    "        # debug prints\n",
    "        # print(x)\n",
    "\n",
    "        # embeddings\n",
    "        ly = self.apply_emb(lS_o, lS_i, self.emb_l)\n",
    "        # debug prints\n",
    "        # print(ly)\n",
    "\n",
    "        # butterfly shuffle (implemented inefficiently for now)\n",
    "        # WARNING: Note that at this point we have the result of the embedding lookup\n",
    "        # for the entire batch on each device. We would like to obtain partial results\n",
    "        # corresponding to all embedding lookups, but part of the batch on each device.\n",
    "        # Therefore, matching the distribution of output of bottom mlp, so that both\n",
    "        # could be used for subsequent interactions on each device.\n",
    "        if len(self.emb_l) != len(ly):\n",
    "            sys.exit(\"ERROR: corrupted intermediate result in parallel_forward call\")\n",
    "\n",
    "        t_list = []\n",
    "        for k, _ in enumerate(self.emb_l):\n",
    "            d = torch.device(\"cuda:\" + str(k % ndevices))\n",
    "            y = scatter(ly[k], device_ids, dim=0)\n",
    "            t_list.append(y)\n",
    "        # adjust the list to be ordered per device\n",
    "        ly = list(map(lambda y: list(y), zip(*t_list)))\n",
    "        # debug prints\n",
    "        # print(ly)\n",
    "\n",
    "        # interactions\n",
    "        z = []\n",
    "        for k in range(ndevices):\n",
    "            zk = self.interact_features(x[k], ly[k])\n",
    "            z.append(zk)\n",
    "        # debug prints\n",
    "        # print(z)\n",
    "\n",
    "        # top mlp\n",
    "        # WARNING: Note that the self.top_l is a list of top mlp modules that\n",
    "        # have been replicated across devices, while z is a list of interaction results\n",
    "        # that by construction are scattered across devices on the first (batch) dim.\n",
    "        # The output is a list of tensors scattered across devices according to the\n",
    "        # distribution of z.\n",
    "        p = parallel_apply(self.top_l_replicas, z, None, device_ids)\n",
    "\n",
    "        ### gather the distributed results ###\n",
    "        p0 = gather(p, self.output_d, dim=0)\n",
    "\n",
    "        # clamp output if needed\n",
    "        if 0.0 < self.loss_threshold and self.loss_threshold < 1.0:\n",
    "            z0 = torch.clamp(\n",
    "                p0, min=self.loss_threshold, max=(1.0 - self.loss_threshold)\n",
    "            )\n",
    "        else:\n",
    "            z0 = p0\n",
    "\n",
    "        return z0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parse arguments ###\n",
    "# args_dict = dict(\n",
    "#     activation_function='relu', arch_embedding_size='4-3-2', arch_interaction_itself=False, arch_interaction_op='dot', arch_mlp_bot='4-3-2', \n",
    "# arch_mlp_top='4-2-1', arch_sparse_feature_size=2, data_generation='random', data_randomize='total', \n",
    "# data_set='kaggle', data_size=1, data_sub_sample_rate=0.0, data_trace_enable_padding=False, \n",
    "# data_trace_file='./input/dist_emb_j.log', debug_mode=False, enable_profiling=False, inference_only=False, \n",
    "# learning_rate=0.01, load_model='', loss_function='mse', loss_threshold=0.0, loss_weights='1.0-1.0', \n",
    "# max_ind_range=-1, md_flag=False, md_round_dims=False, md_temperature=0.3, md_threshold=200, \n",
    "# memory_map=False, mini_batch_size=1, mlperf_acc_threshold=0.0, mlperf_auc_threshold=0.0, \n",
    "# mlperf_bin_loader=False, mlperf_bin_shuffle=False, mlperf_logging=False, nepochs=1, num_batches=0, \n",
    "# num_indices_per_lookup=10, num_indices_per_lookup_fixed=False, num_workers=0, numpy_rand_seed=123, \n",
    "# plot_compute_graph=False, print_freq=1, print_precision=5, print_time=False, processed_data_file='', \n",
    "# qr_collisions=4, qr_flag=False, qr_operation='mult', qr_threshold=200, raw_data_file='', \n",
    "# round_targets=False, save_model='', save_onnx=False, sync_dense_params=True, test_freq=-1, \n",
    "# test_mini_batch_size=-1, test_num_workers=-1, use_gpu=False      \n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parse arguments2 ###\n",
    "args_dict = dict(\n",
    "    activation_function='relu', arch_embedding_size='4-3-2', arch_interaction_itself=False, arch_interaction_op='dot', \n",
    "    arch_mlp_bot='13-512-256-64-16', arch_mlp_top='512-256-1', arch_sparse_feature_size=16, data_generation='dataset', \n",
    "    data_randomize='total', data_set='kaggle', data_size=1, data_sub_sample_rate=0.0, data_trace_enable_padding=False, \n",
    "    data_trace_file='./input/dist_emb_j.log', debug_mode=False, enable_profiling=False, inference_only=False, \n",
    "    learning_rate=0.1, load_model='', loss_function='bce', loss_threshold=0.0, loss_weights='1.0-1.0', \n",
    "    max_ind_range=-1, md_flag=False, md_round_dims=False, md_temperature=0.3, md_threshold=200, memory_map=False, \n",
    "    mini_batch_size=128, mlperf_acc_threshold=0.0, mlperf_auc_threshold=0.0, mlperf_bin_loader=False, \n",
    "    mlperf_bin_shuffle=False, mlperf_logging=False, nepochs=1, num_batches=0, num_indices_per_lookup=10, \n",
    "    num_indices_per_lookup_fixed=False, num_workers=0, numpy_rand_seed=123, plot_compute_graph=False, \n",
    "    print_freq=1024, print_precision=5, print_time=True, processed_data_file='', qr_collisions=4, \n",
    "    qr_flag=False, qr_operation='mult', qr_threshold=200, \n",
    "    raw_data_file='/Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train.txt', round_targets=True, \n",
    "    save_model='', save_onnx=False, sync_dense_params=True, test_freq=-1, test_mini_batch_size=16384, \n",
    "    test_num_workers=16, use_gpu=False     \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation_function': 'relu',\n",
       " 'arch_embedding_size': '4-3-2',\n",
       " 'arch_interaction_itself': False,\n",
       " 'arch_interaction_op': 'dot',\n",
       " 'arch_mlp_bot': '13-512-256-64-16',\n",
       " 'arch_mlp_top': '512-256-1',\n",
       " 'arch_sparse_feature_size': 16,\n",
       " 'data_generation': 'dataset',\n",
       " 'data_randomize': 'total',\n",
       " 'data_set': 'kaggle',\n",
       " 'data_size': 1,\n",
       " 'data_sub_sample_rate': 0.0,\n",
       " 'data_trace_enable_padding': False,\n",
       " 'data_trace_file': './input/dist_emb_j.log',\n",
       " 'debug_mode': False,\n",
       " 'enable_profiling': False,\n",
       " 'inference_only': False,\n",
       " 'learning_rate': 0.1,\n",
       " 'load_model': '',\n",
       " 'loss_function': 'bce',\n",
       " 'loss_threshold': 0.0,\n",
       " 'loss_weights': '1.0-1.0',\n",
       " 'max_ind_range': -1,\n",
       " 'md_flag': False,\n",
       " 'md_round_dims': False,\n",
       " 'md_temperature': 0.3,\n",
       " 'md_threshold': 200,\n",
       " 'memory_map': False,\n",
       " 'mini_batch_size': 128,\n",
       " 'mlperf_acc_threshold': 0.0,\n",
       " 'mlperf_auc_threshold': 0.0,\n",
       " 'mlperf_bin_loader': False,\n",
       " 'mlperf_bin_shuffle': False,\n",
       " 'mlperf_logging': False,\n",
       " 'nepochs': 1,\n",
       " 'num_batches': 0,\n",
       " 'num_indices_per_lookup': 10,\n",
       " 'num_indices_per_lookup_fixed': False,\n",
       " 'num_workers': 0,\n",
       " 'numpy_rand_seed': 123,\n",
       " 'plot_compute_graph': False,\n",
       " 'print_freq': 1024,\n",
       " 'print_precision': 5,\n",
       " 'print_time': True,\n",
       " 'processed_data_file': '',\n",
       " 'qr_collisions': 4,\n",
       " 'qr_flag': False,\n",
       " 'qr_operation': 'mult',\n",
       " 'qr_threshold': 200,\n",
       " 'raw_data_file': '/Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train.txt',\n",
       " 'round_targets': True,\n",
       " 'save_model': '',\n",
       " 'save_onnx': False,\n",
       " 'sync_dense_params': True,\n",
       " 'test_freq': -1,\n",
       " 'test_mini_batch_size': 16384,\n",
       " 'test_num_workers': 16,\n",
       " 'use_gpu': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple class example\n",
    "class test:\n",
    "    def __init__(self, input_dict):\n",
    "        for key in input_dict:\n",
    "            self.__dict__[key] = input_dict[key]\n",
    "            \n",
    "# benchmark\n",
    "class test_benchmark:\n",
    "    def __init__(self, a, b):\n",
    "        self.a=a\n",
    "        self.b=b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = test(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.debug_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change args as needed\n",
    "args.debug_mode = True\n",
    "args.test_freq = 1024\n",
    "args.save_model = '/Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/dummy_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU...\n",
      "Reading raw data=/Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train.txt\n",
      "Skipping counts per file (already exist)\n",
      "Skip existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_0.npz\n",
      "Skip existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_1.npz\n",
      "Skip existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_2.npz\n",
      "Skip existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_3.npz\n",
      "Skip existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_4.npz\n",
      "Skip existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_5.npz\n",
      "Skip existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_6.npz\n",
      "Total number of samples: 458409\n",
      "Divided into days/splits:\n",
      " [65487, 65487, 65487, 65487, 65487, 65487, 65487]\n",
      "Using existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_6_processed.npz\n",
      "Concatenating multiple days into /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/kaggleAdDisplayChallenge_processed.npz file\n",
      "Loaded day: 0 y = 1: 16327 y = 0: 49160\n",
      "Loaded day: 1 y = 1: 33050 y = 0: 97924\n",
      "Loaded day: 2 y = 1: 49487 y = 0: 146974\n",
      "Loaded day: 3 y = 1: 65683 y = 0: 196265\n",
      "Loaded day: 4 y = 1: 82863 y = 0: 244572\n",
      "Loaded day: 5 y = 1: 99500 y = 0: 293422\n",
      "Loaded day: 6 y = 1: 116475 y = 0: 341934\n",
      "Loaded counts!\n",
      "Sparse fea = 26, Dense fea = 13\n",
      "Defined train indices...\n",
      "Randomized indices across days ...\n",
      "Split data according to indices...\n",
      "Reading raw data=/Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train.txt\n",
      "Skipping counts per file (already exist)\n",
      "Skip existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_0.npz\n",
      "Skip existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_1.npz\n",
      "Skip existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_2.npz\n",
      "Skip existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_3.npz\n",
      "Skip existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_4.npz\n",
      "Skip existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_5.npz\n",
      "Skip existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_6.npz\n",
      "Total number of samples: 458409\n",
      "Divided into days/splits:\n",
      " [65487, 65487, 65487, 65487, 65487, 65487, 65487]\n",
      "Using existing /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/train_day_6_processed.npz\n",
      "Concatenating multiple days into /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/kaggleAdDisplayChallenge_processed.npz file\n",
      "Loaded day: 0 y = 1: 16327 y = 0: 49160\n",
      "Loaded day: 1 y = 1: 33050 y = 0: 97924\n",
      "Loaded day: 2 y = 1: 49487 y = 0: 146974\n",
      "Loaded day: 3 y = 1: 65683 y = 0: 196265\n",
      "Loaded day: 4 y = 1: 82863 y = 0: 244572\n",
      "Loaded day: 5 y = 1: 99500 y = 0: 293422\n",
      "Loaded day: 6 y = 1: 116475 y = 0: 341934\n",
      "Loaded counts!\n",
      "Sparse fea = 26, Dense fea = 13\n",
      "Defined test indices...\n",
      "Randomized indices across days ...\n",
      "Split data according to indices...\n"
     ]
    }
   ],
   "source": [
    "### some basic setup ###\n",
    "np.random.seed(args.numpy_rand_seed)\n",
    "np.set_printoptions(precision=args.print_precision)\n",
    "torch.set_printoptions(precision=args.print_precision)\n",
    "torch.manual_seed(args.numpy_rand_seed)\n",
    "\n",
    "if (args.test_mini_batch_size < 0):\n",
    "    # if the parameter is not set, use the training batch size\n",
    "    args.test_mini_batch_size = args.mini_batch_size\n",
    "if (args.test_num_workers < 0):\n",
    "    # if the parameter is not set, use the same parameter for training\n",
    "    args.test_num_workers = args.num_workers\n",
    "\n",
    "use_gpu = args.use_gpu and torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    torch.cuda.manual_seed_all(args.numpy_rand_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    device = torch.device(\"cuda\", 0)\n",
    "    ngpus = torch.cuda.device_count()  # 1\n",
    "    print(\"Using {} GPU(s)...\".format(ngpus))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU...\")\n",
    "\n",
    "### prepare training data ###\n",
    "ln_bot = np.fromstring(args.arch_mlp_bot, dtype=int, sep=\"-\")\n",
    "# input data\n",
    "if (args.data_generation == \"dataset\"):\n",
    "\n",
    "    train_data, train_ld, test_data, test_ld = \\\n",
    "        dp.make_criteo_data_and_loaders(args)\n",
    "    nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)\n",
    "    nbatches_test = len(test_ld)\n",
    "    \n",
    "    # a tensor of vocaburary counts for categorical features\n",
    "    ln_emb = train_data.counts\n",
    "    # enforce maximum limit on number of vectors per embedding\n",
    "    if args.max_ind_range > 0:\n",
    "        ln_emb = np.array(list(map(\n",
    "            lambda x: x if x < args.max_ind_range else args.max_ind_range,\n",
    "            ln_emb\n",
    "        )))\n",
    "    m_den = train_data.m_den\n",
    "    ln_bot[0] = m_den\n",
    "else:\n",
    "    # input and target at random\n",
    "    ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep=\"-\")\n",
    "    m_den = ln_bot[0]\n",
    "    train_data, train_ld = dp.make_random_data_and_loader(args, ln_emb, m_den)\n",
    "    nbatches = args.num_batches if args.num_batches > 0 else len(train_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   988,    542, 181190,  79476,    230,     14,  10417,    443,\n",
       "            3,  23543,   4640, 163044,   3116,     26,   8074, 128285,\n",
       "           10,   3611,   1692,      4, 148667,     15,     14,  29185,\n",
       "           65,  21828], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#litez: the elements represents vocabulary size for each of the embedding element\n",
    "ln_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dlrm_data_pytorch.CriteoDataset at 0x14621fba8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dlrm_data_pytorch.CriteoDataset at 0x14621fba8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ld.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.arch_sparse_feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_emb.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13, 512, 256,  64,  16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3070"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3070"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392960"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3070*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.arch_sparse_feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model arch:\n",
      "mlp top arch 3 layers, with input to output dimensions:\n",
      "[367 512 256   1]\n",
      "# of interactions\n",
      "367\n",
      "mlp bot arch 4 layers, with input to output dimensions:\n",
      "[ 13 512 256  64  16]\n",
      "# of features (sparse and dense)\n",
      "27\n",
      "dense feature size\n",
      "13\n",
      "sparse feature size\n",
      "16\n",
      "# of embeddings (= # of sparse features) 26, with dimensions 16x:\n",
      "[   988    542 181190  79476    230     14  10417    443      3  23543\n",
      "   4640 163044   3116     26   8074 128285     10   3611   1692      4\n",
      " 148667     15     14  29185     65  21828]\n",
      "data (inputs and targets):\n",
      "mini-batch: 0\n",
      "[[1.94591 4.63473 1.79176 ... 2.56495 4.07754 1.09861]\n",
      " [0.      0.      1.94591 ... 0.      0.      1.09861]\n",
      " [0.      5.69709 1.09861 ... 0.69315 0.      1.38629]\n",
      " ...\n",
      " [0.      4.58497 1.09861 ... 0.      0.      0.69315]\n",
      " [0.69315 0.69315 1.09861 ... 0.69315 0.      0.     ]\n",
      " [0.      0.      0.      ... 0.69315 0.      0.69315]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[0, 0, 0, 21, 20, 4, 18, 5, 0, 4, 0, 0, 14, 4, 4, 0, 1, 4, 0, 4, 9, 29, 4, 14, 4, 595, 4, 4, 4, 5, 4, 0, 4, 14, 2, 0, 4, 4, 4, 4, 4, 4, 4, 31, 4, 4, 0, 1, 4, 1, 4, 0, 0, 12, 4, 4, 0, 4, 1, 0, 4, 97, 5, 2, 2, 4, 4, 0, 31, 1, 4, 4, 4, 4, 4, 4, 57, 1, 4, 1, 2, 269, 4, 4, 0, 4, 4, 4, 1, 4, 52, 0, 4, 4, 4, 4, 1, 21, 19, 6, 4, 1, 4, 4, 4, 4, 145, 4, 4, 4, 0, 5, 9, 0, 2, 4, 4, 4, 22, 1, 4, 4, 9, 4, 1, 4, 14, 4], [6, 36, 38, 74, 6, 136, 394, 19, 12, 15, 27, 36, 167, 71, 9, 9, 141, 16, 84, 20, 30, 36, 23, 35, 82, 2, 34, 36, 15, 15, 163, 30, 12, 12, 159, 4, 16, 97, 137, 71, 1, 15, 83, 131, 30, 107, 15, 30, 30, 51, 308, 109, 35, 70, 1, 417, 9, 59, 119, 12, 46, 71, 113, 30, 31, 102, 3, 12, 36, 30, 4, 12, 36, 12, 116, 12, 9, 15, 329, 36, 30, 177, 203, 71, 27, 210, 45, 65, 38, 38, 83, 3, 38, 15, 12, 38, 301, 70, 43, 36, 94, 12, 9, 9, 3, 38, 65, 51, 171, 19, 20, 51, 38, 15, 45, 12, 0, 84, 12, 14, 51, 12, 15, 14, 53, 249, 36, 74], [18015, 5745, 22964, 75405, 7079, 867, 80529, 23, 1314, 18414, 10, 57195, 19, 91961, 150, 150, 1, 93, 3760, 124479, 6909, 23405, 9, 74891, 35238, 63251, 43227, 8267, 52440, 88800, 140577, 1451, 1941, 15823, 113339, 96180, 19, 180, 144391, 90679, 1, 11719, 139, 54477, 264, 10, 106509, 3213, 8112, 87317, 136968, 216, 16950, 152698, 43892, 8455, 150, 4226, 29894, 133197, 9962, 12966, 233, 2048, 787, 1, 140679, 21369, 790, 257, 103228, 33880, 57792, 5172, 42, 69025, 150, 124150, 32231, 84270, 3712, 123346, 35357, 36002, 42, 30571, 216, 1828, 1, 13611, 139, 15606, 17455, 104402, 84094, 1002, 2, 13291, 2576, 57794, 128803, 28908, 150, 150, 51949, 1536, 175, 77173, 38516, 23, 1, 102502, 112551, 49054, 5892, 2618, 6643, 67608, 88470, 17, 92829, 72260, 43869, 17, 5763, 226, 20187, 54530], [11160, 4056, 3026, 2621, 2266, 734, 377, 19, 1084, 11369, 9, 9928, 16, 31605, 136, 136, 1, 82, 2769, 58028, 4809, 14019, 8, 38030, 19985, 10009, 23879, 5635, 978, 3344, 64318, 1188, 1537, 9987, 3031, 4, 16, 159, 1634, 15756, 1, 30, 125, 29124, 233, 9, 36186, 2408, 5536, 65, 63039, 6, 434, 68992, 2600, 66, 136, 253, 17381, 84, 6625, 8384, 207, 1619, 347, 1, 64355, 12936, 675, 227, 4, 19309, 30739, 502, 9, 35636, 136, 4385, 8244, 42049, 2732, 57572, 901, 20357, 9, 201, 6, 118, 1, 8759, 125, 9860, 10865, 50067, 41979, 841, 2, 8576, 1975, 30741, 1590, 16872, 136, 136, 12626, 1242, 156, 1761, 21604, 19, 1, 65, 106, 17349, 4156, 2003, 4642, 34999, 10372, 8, 65, 36971, 24196, 8, 4067, 200, 12308, 29153], [0, 0, 8, 0, 2, 2, 0, 10, 0, 0, 2, 0, 2, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 5, 2, 0, 0, 0, 9, 0, 0, 0, 27, 1, 1, 0, 0, 3, 0, 0, 2, 1, 0, 3, 0, 0, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 8, 0, 0, 0, 0, 0, 2, 0, 0, 0, 58, 0, 0, 0, 0, 0, 3, 5, 2, 1, 2, 2, 17, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3], [1, 0, 3, 1, 0, 1, 2, 3, 1, 1, 0, 0, 2, 3, 0, 0, 0, 0, 1, 2, 2, 6, 0, 3, 3, 1, 0, 3, 3, 5, 0, 1, 0, 1, 3, 1, 1, 2, 0, 2, 0, 5, 5, 1, 5, 3, 1, 1, 0, 0, 4, 2, 3, 0, 1, 0, 1, 5, 3, 2, 0, 2, 2, 1, 5, 0, 1, 3, 6, 0, 2, 4, 1, 1, 3, 1, 1, 3, 2, 1, 5, 2, 2, 4, 1, 1, 2, 1, 1, 0, 0, 0, 3, 1, 0, 2, 0, 0, 2, 0, 3, 2, 0, 0, 3, 0, 0, 2, 3, 3, 4, 0, 1, 3, 0, 2, 3, 3, 3, 1, 2, 0, 4, 1, 2, 0, 0, 1], [533, 97, 336, 1018, 117, 840, 299, 381, 4625, 19, 754, 955, 2707, 6815, 1345, 100, 1335, 1653, 655, 23, 7101, 403, 150, 347, 2069, 6895, 690, 1748, 344, 1422, 2998, 2387, 643, 959, 273, 503, 4905, 5681, 2453, 2924, 54, 1143, 2564, 2537, 1282, 2335, 1709, 667, 2129, 222, 887, 597, 6268, 178, 690, 1826, 1397, 196, 7243, 196, 507, 1571, 292, 1641, 400, 492, 607, 503, 4693, 3037, 1317, 1921, 64, 3168, 738, 54, 4446, 593, 946, 9019, 196, 553, 1154, 1615, 5913, 426, 2054, 774, 215, 954, 1157, 211, 283, 1497, 2987, 1857, 6042, 178, 4894, 63, 23, 949, 171, 10, 661, 322, 5620, 4523, 8355, 404, 7155, 1332, 1673, 513, 2890, 1279, 6049, 4222, 612, 36, 485, 309, 1480, 125, 2201, 253, 989, 589], [1, 1, 1, 3, 1, 1, 1, 3, 3, 7, 3, 1, 1, 3, 1, 1, 3, 1, 3, 1, 3, 3, 1, 3, 1, 1, 5, 3, 3, 5, 3, 1, 0, 1, 1, 0, 1, 3, 1, 3, 1, 5, 1, 3, 1, 1, 0, 0, 1, 1, 4, 3, 1, 3, 1, 9, 3, 1, 3, 1, 1, 0, 1, 3, 1, 6, 3, 1, 1, 6, 0, 3, 1, 6, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 3, 1, 17, 5, 15, 0, 1, 1, 3, 1, 1, 1, 1, 5, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 0, 1, 0, 3, 3, 3, 3, 3, 1, 0, 15, 3, 1, 3, 3, 5], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [10, 477, 15516, 734, 939, 50, 2468, 26, 25, 1316, 596, 1215, 12514, 2043, 10, 362, 1807, 1884, 1964, 10, 9689, 690, 9538, 2120, 848, 7494, 2900, 728, 1007, 1241, 10, 1589, 10, 10, 734, 10, 10, 3529, 248, 16939, 5119, 5314, 1278, 7443, 15283, 4022, 40, 4877, 4105, 50, 5933, 10, 7652, 597, 8333, 3146, 1217, 10660, 1453, 2459, 1548, 19082, 10, 348, 2041, 1620, 5, 10, 5146, 2977, 1125, 2929, 11193, 10, 3667, 5376, 8489, 4642, 10, 5794, 12618, 600, 1690, 5891, 10, 8080, 5, 1412, 515, 10, 68, 1284, 220, 1320, 1750, 10, 9449, 5789, 19278, 394, 1296, 781, 2344, 10, 10, 155, 10, 10, 10, 95, 7717, 5728, 2837, 15, 10, 1428, 7878, 6662, 1559, 30, 286, 1112, 296, 283, 10, 10, 821, 13162], [445, 90, 293, 660, 8, 219, 267, 112, 2722, 18, 1022, 765, 1088, 926, 1039, 57, 1761, 1105, 538, 22, 2948, 90, 137, 206, 1355, 3252, 567, 1274, 301, 1090, 1960, 855, 530, 375, 244, 421, 2843, 2142, 1087, 1172, 50, 287, 850, 399, 989, 1613, 18, 548, 2341, 526, 718, 494, 2469, 164, 567, 164, 1075, 180, 2814, 180, 190, 1030, 260, 381, 263, 411, 44, 421, 2755, 1984, 1023, 632, 59, 50, 108, 50, 747, 490, 761, 4187, 180, 460, 219, 690, 2730, 50, 103, 1465, 196, 764, 158, 192, 253, 1128, 1952, 1340, 3247, 164, 1502, 58, 22, 686, 127, 10, 543, 180, 3106, 2675, 2787, 112, 2407, 1032, 991, 850, 2983, 998, 2986, 2560, 206, 35, 53, 53, 218, 112, 3821, 228, 730, 486], [16409, 5249, 20907, 68706, 6450, 845, 5913, 22, 1268, 16778, 10, 52256, 19, 83493, 149, 149, 1, 92, 3472, 112570, 6297, 21320, 9, 68235, 32086, 57650, 39352, 7530, 47787, 80688, 126929, 1394, 1833, 14409, 102617, 87296, 19, 178, 130289, 82363, 1, 10678, 138, 49696, 261, 10, 96528, 2977, 7383, 74, 123816, 213, 15439, 137681, 39965, 7705, 149, 3893, 27287, 120395, 9073, 11808, 230, 1931, 768, 1, 127020, 19467, 771, 254, 627, 30864, 52786, 4727, 41, 62943, 149, 112285, 29442, 76762, 3429, 111564, 32194, 32781, 41, 27918, 213, 1728, 1, 12391, 138, 14211, 15900, 94631, 76612, 974, 2, 12102, 2407, 52788, 116451, 26361, 149, 149, 26849, 1465, 173, 70296, 35071, 22, 1, 74, 6023, 44659, 5386, 2442, 6059, 61628, 80393, 17, 74, 65873, 39946, 17, 5267, 223, 18393, 49747], [406, 87, 274, 48, 8, 204, 250, 107, 1328, 18, 563, 695, 957, 828, 918, 57, 847, 970, 491, 22, 2348, 87, 132, 196, 1173, 2542, 519, 1109, 282, 959, 1646, 767, 484, 346, 228, 23, 2277, 1766, 579, 1026, 50, 269, 18, 369, 878, 1375, 18, 501, 1287, 48, 652, 452, 2011, 158, 519, 158, 946, 10, 2253, 10, 182, 909, 243, 351, 246, 378, 44, 23, 2214, 1662, 902, 577, 59, 50, 103, 50, 677, 448, 691, 2124, 10, 419, 204, 627, 2199, 50, 44, 577, 135, 694, 152, 184, 236, 992, 1639, 1162, 2539, 158, 1286, 58, 22, 623, 122, 10, 496, 10, 2449, 2161, 2235, 107, 1965, 911, 880, 18, 1605, 886, 2373, 2081, 196, 35, 53, 53, 203, 107, 1041, 213, 664, 444], [5, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 0, 2, 1, 2, 5, 2, 2, 7, 2, 2, 1, 8, 2, 0, 0, 2, 1, 6, 12, 2, 5, 2, 2, 1, 1, 2, 0, 5, 0, 2, 0, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 0, 2, 1, 1, 1, 2, 2, 5, 2, 2, 2, 4, 1, 2, 5, 1, 2, 0, 0, 0, 2, 1, 2, 2, 2, 5, 2, 2, 4, 1, 2, 2, 1, 5, 1, 1, 1, 0, 2, 2, 2, 2, 1, 1, 0, 1, 1, 1, 1, 2, 1, 2, 1, 4, 0, 2, 0, 1, 0, 8, 2, 7, 2, 1, 4, 0, 1, 0, 6, 3, 2, 2, 2, 1], [498, 54, 58, 129, 6, 302, 2292, 247, 1839, 278, 189, 506, 590, 222, 156, 10, 1641, 808, 196, 932, 154, 506, 384, 533, 537, 1101, 260, 506, 758, 816, 1986, 154, 1055, 184, 401, 236, 366, 181, 1021, 353, 1, 19, 2028, 373, 249, 206, 19, 821, 529, 457, 1351, 211, 57, 1653, 1, 2354, 172, 98, 3927, 627, 73, 353, 1146, 154, 528, 365, 11, 3981, 530, 190, 496, 5926, 389, 2483, 312, 771, 10, 1338, 1426, 530, 154, 1167, 1156, 349, 81, 630, 210, 1015, 58, 58, 571, 79, 152, 19, 2388, 152, 2115, 150, 66, 54, 1084, 3464, 172, 10, 11, 94, 137, 322, 2201, 25, 29, 457, 136, 285, 566, 2745, 3049, 449, 3904, 18, 84, 6067, 308, 22, 4035, 851, 506, 129], [14220, 4702, 17938, 2961, 5751, 799, 409, 22, 1187, 14533, 10, 12480, 19, 67786, 147, 147, 1, 91, 3131, 90002, 5611, 18292, 9, 55971, 27126, 47685, 33049, 6686, 39845, 38863, 100904, 1303, 1702, 12560, 82420, 35, 19, 173, 3665, 20807, 1, 34, 136, 41370, 251, 10, 77787, 2697, 6558, 74, 98579, 207, 468, 109163, 2932, 6832, 147, 276, 23250, 93, 8012, 10339, 223, 1793, 730, 1, 100970, 16756, 733, 245, 4, 26123, 43874, 3469, 41, 51905, 147, 89789, 10161, 62632, 3091, 89231, 1916, 27674, 41, 812, 207, 128, 1, 10840, 136, 12386, 13784, 76331, 62520, 919, 2, 10596, 2210, 43876, 1762, 22482, 147, 147, 16309, 1367, 169, 1960, 29525, 22, 1, 74, 5372, 37320, 4825, 2240, 5405, 50867, 65409, 17, 74, 54170, 33539, 17, 4717, 216, 15863, 41413], [1, 0, 3, 0, 1, 1, 2, 0, 0, 0, 2, 0, 2, 7, 8, 0, 3, 3, 1, 8, 0, 0, 3, 0, 0, 0, 0, 0, 3, 2, 3, 0, 2, 8, 0, 4, 8, 3, 7, 2, 0, 0, 7, 2, 0, 0, 1, 3, 0, 1, 0, 0, 0, 0, 0, 6, 0, 3, 7, 3, 0, 0, 0, 0, 0, 0, 0, 4, 2, 0, 2, 2, 7, 2, 0, 0, 0, 0, 8, 7, 2, 0, 3, 2, 0, 1, 0, 0, 3, 8, 2, 6, 3, 0, 2, 2, 2, 0, 6, 0, 0, 0, 0, 4, 3, 0, 8, 0, 0, 0, 0, 0, 3, 7, 0, 0, 3, 2, 0, 3, 2, 2, 2, 7, 8, 3, 3, 3], [6, 45, 48, 103, 6, 221, 1287, 24, 218, 203, 67, 45, 293, 98, 106, 10, 979, 19, 116, 26, 119, 45, 75, 44, 114, 64, 43, 45, 494, 203, 285, 119, 671, 141, 272, 4, 19, 139, 653, 98, 1, 18, 613, 206, 38, 157, 18, 149, 50, 70, 821, 160, 47, 983, 1, 1319, 120, 82, 181, 84, 61, 98, 168, 119, 39, 150, 11, 2057, 45, 119, 336, 2824, 45, 308, 172, 502, 10, 204, 861, 45, 119, 430, 725, 98, 67, 410, 60, 251, 48, 48, 375, 11, 48, 18, 1336, 48, 714, 97, 56, 45, 689, 632, 120, 10, 11, 79, 92, 70, 556, 24, 26, 70, 48, 18, 60, 791, 1305, 116, 1882, 17, 70, 2882, 18, 21, 872, 546, 45, 103], [923, 0, 0, 1, 233, 0, 1, 0, 1, 0, 1, 178, 257, 229, 1, 1, 1, 0, 0, 0, 0, 161, 1, 192, 161, 0, 229, 178, 0, 245, 1, 0, 1, 1, 0, 1, 0, 0, 1, 571, 1, 10, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 43, 1, 379, 1, 1, 39, 1, 0, 0, 0, 0, 0, 1, 69, 1, 159, 0, 1, 1, 208, 1, 0, 1, 1, 71, 1, 199, 0, 1, 1, 493, 1, 0, 0, 1, 0, 0, 1, 0, 0, 126, 1, 0, 0, 117, 91, 578, 1, 1, 1, 1, 302, 0, 1, 1, 0, 0, 0, 1, 0, 12, 70, 1, 0, 91, 1, 1, 1, 1, 507, 1, 1, 0, 36, 1], [3, 3, 2, 1, 0, 2, 1, 3, 1, 2, 1, 0, 2, 3, 1, 1, 1, 2, 2, 0, 2, 3, 1, 3, 3, 3, 3, 0, 0, 0, 1, 3, 1, 1, 2, 1, 2, 2, 1, 3, 1, 3, 1, 0, 0, 2, 3, 3, 3, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 2, 3, 3, 3, 3, 1, 0, 1, 0, 3, 1, 1, 3, 1, 2, 1, 1, 2, 1, 3, 3, 1, 1, 0, 1, 2, 2, 1, 2, 2, 1, 3, 0, 3, 1, 2, 2, 0, 3, 0, 1, 1, 1, 1, 3, 0, 1, 1, 2, 2, 0, 1, 2, 0, 3, 1, 3, 3, 1, 1, 1, 1, 3, 1, 1, 0, 0, 1], [15481, 4994, 19645, 63567, 6122, 827, 423, 22, 1228, 15825, 10, 13531, 19, 77023, 148, 148, 1, 92, 3315, 103264, 5974, 20027, 9, 63135, 30007, 53557, 36729, 7143, 44482, 43343, 116131, 1353, 1775, 13618, 94355, 243, 19, 177, 119178, 75994, 1, 10112, 137, 46253, 259, 10, 88818, 2846, 7002, 74, 113348, 212, 14582, 125866, 37304, 7308, 148, 1009, 25621, 94, 8598, 11163, 229, 1870, 753, 1, 116210, 18316, 756, 253, 4, 28886, 49137, 4500, 41, 58379, 148, 103006, 27581, 70940, 3273, 102352, 30106, 30639, 41, 9025, 212, 1676, 1, 11720, 137, 13433, 15005, 87083, 70809, 950, 2, 11443, 2319, 49139, 106719, 24747, 148, 148, 25207, 1422, 172, 65029, 32746, 22, 1, 74, 5716, 41604, 5122, 2351, 5751, 57166, 74206, 17, 74, 61023, 37285, 17, 5010, 222, 17318, 46300], [2, 1, 0, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 7, 2, 5, 3, 2, 2, 2, 2, 2, 0, 0, 7, 3, 5, 3, 3, 0, 1, 4, 2, 3, 0, 2, 2, 1, 1, 3, 2, 0, 0, 0, 2, 2, 2, 2, 2, 1, 0, 3, 2, 3, 1, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 3, 1, 1, 5, 2, 2, 2, 1, 0, 2, 4, 1, 1, 0, 2, 2, 2, 3, 2, 7, 7, 2, 2, 1, 2, 5, 0, 2, 6, 0, 2, 0, 2, 0, 2, 2, 1, 2, 0, 0, 2, 2, 2, 0, 3, 7, 1, 2, 5, 7, 5, 2, 2, 9, 6, 4, 2, 2, 7, 0, 2, 3, 2, 2, 2, 2, 0, 2, 7, 2, 4, 5, 3, 1, 2], [1335, 12, 89, 798, 345, 157, 287, 3, 775, 6236, 9, 5582, 16, 12499, 105, 105, 1, 67, 12, 5648, 2948, 7392, 8, 12, 12, 4091, 11389, 12, 652, 2120, 348, 849, 1074, 124, 1989, 4, 16, 123, 229, 8133, 1, 25, 95, 4184, 175, 9, 15840, 1607, 2299, 4, 528, 6, 38, 1837, 649, 49, 105, 38, 12, 4, 3898, 4833, 156, 1124, 63, 1, 5976, 6907, 12, 172, 4, 9643, 13954, 376, 9, 122, 105, 2713, 1524, 17885, 1806, 679, 662, 6677, 9, 153, 6, 77, 1, 1366, 95, 450, 6011, 12, 2967, 141, 2, 12, 1342, 12, 121, 985, 105, 105, 1180, 63, 77, 11, 1369, 3, 1, 4, 85, 1398, 12, 1358, 2860, 12, 5780, 8, 4, 157, 12, 8, 1555, 152, 6641, 1983], [6, 11, 5, 1, 6, 0, 1, 0, 1, 2, 1, 11, 2, 14, 1, 1, 1, 4, 2, 2, 5, 11, 1, 5, 0, 2, 10, 11, 2, 2, 1, 5, 1, 1, 5, 1, 2, 2, 1, 14, 1, 2, 1, 9, 5, 0, 2, 5, 5, 1, 1, 1, 5, 7, 1, 7, 1, 1, 0, 1, 14, 14, 15, 5, 5, 1, 0, 1, 11, 5, 1, 1, 11, 1, 0, 1, 1, 2, 1, 11, 5, 1, 1, 14, 1, 10, 4, 1, 5, 5, 1, 0, 5, 2, 1, 5, 27, 7, 10, 11, 1, 1, 1, 1, 0, 5, 1, 1, 2, 0, 2, 1, 5, 14, 10, 1, 0, 22, 1, 1, 1, 1, 2, 1, 1, 7, 11, 1], [1004, 20, 101, 1, 543, 423, 1, 14, 1, 167, 1, 4280, 283, 9633, 1, 1, 1, 1207, 20, 4338, 1535, 5668, 1, 20, 20, 220, 8804, 476, 533, 1636, 1, 1098, 1, 1, 510, 1, 184, 14, 1, 6250, 1, 24, 1, 3236, 144, 200, 3668, 1217, 735, 1, 1, 1, 34, 596, 1, 52, 1, 1, 20, 1, 19, 3727, 909, 2196, 59, 1, 6976, 1, 20, 1694, 1, 1, 10759, 1, 193, 1, 1, 168, 1, 12528, 891, 1, 1, 5113, 1, 366, 5, 1, 161, 387, 1, 342, 857, 20, 1, 165, 109, 20, 1008, 131, 1, 1, 1, 1, 884, 59, 1, 1, 3401, 14, 7469, 1, 12, 6814, 20, 1, 2176, 11855, 1, 1, 1, 1, 20, 1, 1, 125, 5093, 1]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "mini-batch: 1\n",
      "[[2.19722 0.      1.09861 ... 0.69315 0.      3.2581 ]\n",
      " [0.      1.94591 0.      ... 0.69315 0.      0.     ]\n",
      " [0.69315 0.      2.99573 ... 0.69315 0.      1.60944]\n",
      " ...\n",
      " [0.      0.      1.79176 ... 1.79176 0.      0.69315]\n",
      " [0.      0.      0.      ... 0.69315 0.      0.     ]\n",
      " [1.09861 0.      0.      ... 0.69315 0.      0.     ]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[23, 1, 0, 30, 4, 1, 4, 4, 1, 4, 2, 0, 6, 28, 1, 4, 2, 35, 4, 1, 4, 4, 9, 21, 4, 4, 0, 5, 12, 4, 1, 3, 2, 4, 4, 4, 4, 4, 4, 4, 5, 5, 2, 0, 4, 3, 1, 9, 4, 0, 1, 4, 0, 22, 4, 4, 4, 4, 5, 4, 0, 1, 3, 4, 4, 4, 0, 0, 9, 4, 2, 4, 0, 21, 0, 5, 2, 2, 4, 4, 0, 1, 1, 9, 1, 0, 0, 3, 4, 4, 4, 4, 1, 2, 35, 4, 4, 4, 4, 4, 21, 0, 0, 4, 0, 4, 0, 0, 4, 1, 4, 0, 4, 3, 4, 4, 0, 4, 0, 22, 21, 4, 0, 4, 4, 0, 30, 4], [160, 51, 100, 35, 47, 12, 24, 14, 31, 131, 118, 0, 346, 23, 12, 16, 51, 9, 12, 12, 34, 36, 39, 342, 299, 51, 12, 2, 56, 348, 18, 2, 118, 27, 127, 36, 52, 217, 12, 12, 0, 259, 12, 51, 0, 208, 38, 12, 12, 12, 35, 30, 65, 36, 1, 399, 37, 59, 12, 102, 125, 71, 392, 492, 397, 51, 18, 36, 115, 12, 4, 9, 35, 236, 38, 35, 70, 12, 100, 82, 167, 38, 269, 198, 74, 51, 12, 3, 273, 9, 12, 34, 199, 20, 41, 71, 35, 216, 10, 27, 149, 2, 150, 36, 38, 28, 16, 154, 12, 6, 12, 0, 78, 37, 16, 305, 332, 365, 52, 70, 36, 384, 18, 25, 175, 102, 48, 30], [136876, 143059, 104224, 1224, 118343, 8159, 34088, 17, 94207, 951, 253, 32, 90709, 17, 98269, 155999, 97887, 150, 98353, 2178, 27813, 12373, 53, 17, 40905, 144940, 42963, 226, 53157, 3466, 5531, 1, 253, 71, 297, 919, 100396, 1, 493, 51923, 32, 49736, 73809, 106252, 6643, 2, 7695, 7543, 97536, 468, 7629, 264, 107, 55062, 20617, 5896, 649, 1134, 2970, 96145, 70057, 145482, 5631, 35644, 9, 95031, 1641, 64973, 20245, 92419, 33456, 134306, 49210, 9891, 2077, 169, 123270, 68203, 14446, 13816, 64894, 2093, 63849, 137413, 103980, 2613, 95048, 3, 1698, 150, 27533, 32278, 10, 48836, 85214, 84480, 18325, 982, 12, 10, 152834, 601, 274, 123895, 7893, 1672, 48106, 466, 5002, 136, 1, 87485, 29949, 218, 72482, 1, 7572, 9, 67, 75595, 54525, 145324, 117982, 142076, 20356, 200, 10283, 7801], [3069, 65, 49995, 1020, 55600, 5567, 19420, 8, 45958, 798, 224, 3, 1155, 8, 10577, 70215, 65, 136, 24055, 575, 16295, 8046, 46, 8, 1155, 65, 84, 200, 3509, 2576, 18, 1, 224, 9, 260, 769, 66, 1, 430, 84, 3, 2733, 1165, 65, 4642, 2, 5284, 430, 8307, 407, 5243, 233, 95, 29417, 12534, 4159, 557, 80, 2249, 178, 24372, 66212, 3978, 20173, 8, 65, 18, 33804, 3029, 45253, 4, 699, 2289, 941, 1644, 152, 57546, 35277, 9201, 8870, 823, 1657, 9495, 63193, 35292, 65, 46319, 3, 1363, 136, 84, 18597, 9, 7249, 42400, 42137, 11320, 822, 11, 9, 64, 519, 240, 66, 5397, 1343, 26108, 405, 3551, 122, 1, 915, 17412, 193, 37060, 1, 2317, 8, 58, 38330, 29150, 3790, 671, 61241, 1037, 178, 66, 5353], [0, 2, 0, 2, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 2, 2, 0, 0, 5, 0, 2, 0, 2, 1, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 6, 0, 0, 111, 1, 6, 0, 0, 0, 0, 3, 2, 1, 0, 3, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 14, 1, 0, 2, 0, 0, 4, 0, 3, 7, 5, 80, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 27, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 6, 0, 0, 16, 0, 0, 0, 2, 0, 0, 2, 2, 2, 0, 0, 0, 0, 1, 1], [2, 2, 1, 3, 1, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 1, 3, 0, 3, 0, 0, 2, 0, 1, 0, 2, 1, 1, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 5, 1, 2, 6, 0, 0, 0, 0, 2, 1, 0, 0, 2, 1, 5, 1, 1, 5, 0, 1, 1, 2, 4, 3, 0, 1, 1, 2, 2, 1, 3, 1, 0, 2, 0, 2, 1, 0, 1, 1, 0, 0, 6, 5, 1, 0, 0, 2, 0, 0, 3, 2, 2, 0, 2, 3, 1, 0, 0, 1, 0, 0, 0, 0, 5, 1, 1, 0, 2, 6, 3], [1503, 368, 5214, 215, 316, 503, 423, 1416, 8278, 1186, 369, 3208, 7078, 982, 774, 2885, 100, 539, 4609, 132, 1039, 111, 61, 458, 617, 1656, 334, 6902, 6988, 2315, 446, 19, 369, 241, 774, 97, 1227, 1040, 54, 503, 7186, 867, 63, 2052, 134, 4013, 368, 850, 5883, 1279, 1355, 1492, 2228, 63, 54, 2879, 3633, 612, 1879, 32, 1018, 2625, 411, 1219, 1793, 1541, 1326, 893, 24, 6031, 20, 2048, 175, 24, 362, 6744, 3260, 63, 2784, 3936, 52, 241, 3659, 770, 810, 1867, 23, 11, 241, 2163, 2299, 426, 147, 3422, 288, 2219, 56, 5043, 207, 868, 1027, 498, 111, 2572, 171, 682, 4901, 249, 533, 396, 1573, 3548, 3576, 1955, 3675, 1278, 4426, 5293, 4484, 253, 983, 257, 930, 35, 241, 850, 2003, 3288], [1, 1, 1, 1, 1, 1, 1, 3, 0, 1, 1, 1, 2, 1, 1, 1, 5, 1, 1, 1, 1, 1, 9, 1, 1, 3, 0, 13, 3, 1, 1, 3, 1, 1, 3, 0, 13, 1, 1, 6, 1, 3, 1, 5, 0, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 33, 1, 1, 9, 1, 1, 3, 1, 3, 1, 1, 1, 3, 1, 1, 1, 3, 1, 3, 1, 0, 1, 3, 1, 1, 1, 1, 1, 23, 1, 1, 1, 1, 0, 1, 1, 5, 3, 1, 0, 1, 3, 1, 3, 3, 3, 1, 1, 3, 4, 1, 1, 3, 1, 1, 2, 1, 5, 1, 5, 110, 1, 1, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [2013, 50, 2305, 515, 536, 10, 10, 22, 10, 1005, 11038, 3024, 10, 10, 2617, 10, 612, 8608, 6196, 161, 1253, 54, 51, 5, 10, 130, 10, 10, 13011, 2163, 1174, 3447, 679, 189, 2617, 705, 10, 10, 1242, 10, 3752, 1653, 394, 79, 753, 2048, 50, 13096, 10, 1726, 9893, 50, 10, 5465, 874, 10, 3900, 959, 33, 650, 14878, 6948, 142, 2215, 10, 69, 1580, 8767, 31, 17122, 16, 10, 366, 1955, 4186, 12912, 389, 1143, 8009, 55, 2464, 50, 10, 54, 734, 130, 23, 518, 3638, 1557, 10, 1242, 50, 5343, 2858, 1701, 308, 10, 193, 4, 367, 12911, 54, 10, 130, 2870, 632, 2030, 10, 15941, 1898, 10, 10, 65, 15130, 1981, 10, 552, 270, 1171, 8643, 10, 10, 191, 556, 342, 4866, 3995], [1133, 660, 263, 196, 189, 23, 357, 27, 3978, 63, 15, 336, 3599, 348, 632, 641, 57, 450, 2112, 119, 50, 59, 56, 462, 509, 403, 292, 1930, 3560, 1603, 288, 18, 15, 219, 632, 90, 960, 599, 50, 421, 2322, 705, 58, 92, 737, 1509, 660, 690, 3194, 998, 339, 1125, 1561, 1751, 50, 84, 1094, 206, 37, 31, 526, 690, 626, 863, 313, 1158, 218, 723, 23, 517, 19, 1456, 161, 23, 315, 2603, 2102, 58, 1850, 2360, 22, 219, 1456, 629, 661, 1032, 22, 3, 219, 1525, 1594, 50, 134, 2179, 257, 621, 36, 2027, 189, 309, 642, 416, 59, 1751, 127, 37, 3898, 225, 445, 342, 632, 2249, 2264, 55, 2933, 203, 957, 2634, 2656, 228, 659, 231, 748, 34, 219, 690, 879, 2122], [123738, 74, 94473, 1187, 107056, 7429, 31046, 17, 85507, 924, 250, 31, 82390, 17, 89165, 140597, 74, 149, 89239, 2050, 25347, 11274, 52, 17, 37239, 74, 39110, 223, 48457, 3210, 5053, 1, 250, 70, 294, 893, 91046, 1, 484, 47308, 31, 45291, 67257, 74, 6059, 2, 7006, 6868, 88518, 459, 6948, 261, 106, 50250, 18786, 5389, 634, 1103, 2765, 87266, 63888, 131280, 5144, 32458, 9, 74, 1563, 59228, 18449, 83901, 30500, 121426, 44800, 9013, 1960, 168, 111495, 62178, 13152, 12580, 59154, 1975, 58193, 124194, 94252, 74, 86260, 3, 1615, 149, 25089, 29486, 10, 44459, 77592, 76954, 16695, 955, 12, 10, 137795, 589, 271, 112059, 7187, 1590, 43792, 457, 4575, 135, 1, 39826, 27340, 215, 66073, 1, 6895, 9, 66, 68872, 49742, 16526, 40253, 128252, 18554, 197, 9374, 7104], [995, 48, 246, 135, 181, 23, 332, 27, 2906, 63, 15, 314, 298, 325, 577, 586, 57, 411, 1753, 114, 50, 59, 56, 358, 466, 373, 273, 1621, 2716, 1367, 78, 18, 15, 204, 577, 87, 415, 548, 50, 23, 1900, 641, 58, 89, 116, 1292, 48, 627, 2505, 886, 316, 989, 1332, 58, 50, 82, 963, 196, 37, 31, 48, 627, 327, 775, 294, 1013, 203, 657, 23, 473, 19, 1254, 155, 23, 296, 2115, 1746, 58, 1556, 1933, 22, 204, 1254, 574, 307, 911, 22, 3, 204, 1303, 1359, 50, 129, 1791, 240, 567, 36, 1693, 181, 290, 587, 383, 59, 58, 122, 37, 1135, 210, 406, 319, 577, 1846, 1855, 55, 1886, 193, 141, 532, 2148, 213, 112, 136, 678, 34, 204, 627, 471, 1757], [12, 1, 0, 1, 1, 1, 2, 0, 0, 2, 3, 0, 2, 1, 2, 1, 1, 1, 1, 2, 0, 0, 0, 2, 2, 1, 2, 1, 2, 2, 0, 1, 3, 1, 1, 5, 2, 2, 1, 2, 0, 2, 1, 1, 1, 1, 2, 1, 7, 0, 2, 2, 2, 2, 11, 0, 6, 1, 2, 2, 1, 5, 1, 2, 2, 1, 1, 0, 2, 0, 3, 1, 2, 0, 1, 1, 1, 1, 2, 2, 2, 1, 4, 1, 1, 1, 0, 3, 4, 1, 2, 1, 1, 1, 2, 2, 2, 20, 1, 2, 2, 0, 1, 2, 2, 2, 0, 14, 0, 1, 2, 1, 0, 6, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 1], [967, 84, 332, 53, 7100, 2965, 169, 2260, 46, 373, 997, 7, 1552, 384, 501, 348, 84, 156, 2299, 3430, 260, 389, 59, 3067, 1122, 84, 413, 825, 93, 1569, 772, 825, 997, 42, 3255, 530, 85, 667, 771, 532, 7, 2525, 1860, 457, 2679, 628, 94, 870, 3205, 5788, 126, 329, 113, 506, 123, 2546, 750, 98, 1414, 197, 1447, 353, 2092, 4486, 2279, 457, 723, 389, 5754, 6561, 874, 2519, 1433, 738, 554, 1491, 2558, 180, 956, 537, 685, 58, 1061, 582, 129, 457, 5492, 28, 977, 172, 4005, 51, 583, 1327, 450, 222, 205, 666, 12, 43, 492, 470, 360, 431, 315, 562, 665, 775, 3857, 142, 1421, 108, 157, 213, 166, 1182, 1448, 3843, 85, 357, 506, 4607, 2105, 102, 792, 197, 76, 580], [98519, 74, 76219, 1115, 85781, 6595, 26266, 17, 69325, 871, 242, 31, 2860, 17, 72143, 111390, 74, 147, 72190, 621, 21629, 9892, 52, 17, 2354, 74, 93, 216, 25377, 2901, 239, 1, 242, 70, 283, 841, 73570, 1, 464, 93, 31, 37831, 55228, 74, 5405, 2, 6237, 3316, 71644, 439, 6187, 251, 104, 41832, 16184, 4828, 603, 89, 2515, 70700, 52638, 104245, 4613, 27416, 9, 74, 1456, 48942, 3450, 68101, 35, 96740, 37443, 1896, 1820, 165, 89179, 51304, 11478, 10989, 901, 1834, 11878, 98853, 76052, 74, 69915, 3, 1503, 147, 93, 25016, 10, 8821, 63248, 62776, 14466, 900, 12, 10, 73, 561, 261, 89617, 6388, 1480, 36613, 437, 4101, 133, 1, 33440, 23295, 209, 54321, 1, 6142, 9, 66, 56467, 41409, 14321, 6575, 95520, 15997, 192, 8274, 6324], [0, 7, 2, 0, 0, 4, 8, 0, 8, 0, 0, 3, 0, 4, 0, 8, 0, 7, 2, 3, 7, 0, 7, 0, 8, 0, 0, 7, 0, 5, 0, 0, 0, 2, 0, 0, 3, 8, 0, 4, 3, 0, 0, 6, 0, 3, 2, 0, 8, 2, 0, 0, 0, 0, 0, 7, 2, 0, 6, 3, 2, 7, 1, 6, 0, 2, 0, 0, 5, 3, 0, 3, 0, 1, 2, 0, 2, 0, 2, 0, 1, 2, 0, 5, 5, 6, 0, 0, 0, 2, 2, 1, 0, 3, 6, 0, 0, 7, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 1, 3, 8, 0, 0, 0, 2, 3, 0, 3, 0, 0, 2, 1, 7, 0, 0, 2], [273, 70, 146, 44, 3270, 1431, 128, 1121, 39, 206, 180, 7, 928, 75, 338, 126, 70, 106, 338, 949, 43, 45, 49, 1638, 708, 70, 87, 64, 78, 938, 503, 64, 180, 34, 198, 45, 71, 434, 502, 84, 7, 975, 1090, 70, 1305, 408, 79, 559, 1314, 2426, 44, 38, 92, 45, 100, 1231, 488, 82, 856, 150, 266, 98, 1199, 2266, 1223, 70, 472, 45, 1102, 3082, 563, 327, 864, 481, 367, 127, 533, 138, 146, 114, 447, 48, 675, 382, 103, 70, 2660, 3, 627, 120, 87, 43, 383, 26, 53, 98, 156, 433, 12, 34, 332, 2, 253, 292, 148, 35, 126, 264, 1120, 111, 281, 0, 109, 162, 126, 736, 870, 1008, 71, 97, 45, 1161, 606, 32, 513, 150, 63, 161], [1, 1, 30, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 21, 161, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 91, 49, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 43, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 382, 1, 1, 1, 1, 31, 1, 0, 44, 43, 1, 188, 237, 0, 0, 1, 229, 1, 1, 1, 2, 1, 1, 1, 134, 1, 0, 0, 57, 258, 1, 0, 1, 1, 0, 0, 175, 0, 372, 0, 1, 1, 0, 1, 0, 355, 1, 0, 1, 1, 1, 0, 575, 0, 1, 1, 21, 1, 1, 0, 0], [1, 1, 0, 3, 1, 1, 2, 1, 2, 3, 3, 0, 1, 1, 1, 2, 1, 1, 1, 1, 3, 3, 0, 1, 1, 1, 1, 0, 1, 2, 1, 2, 0, 1, 2, 3, 0, 1, 1, 1, 2, 1, 1, 1, 0, 2, 0, 1, 1, 1, 3, 0, 1, 3, 1, 0, 1, 1, 1, 1, 0, 0, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 0, 1, 3, 2, 0, 1, 0, 3, 3, 2, 1, 3, 1, 1, 1, 2, 1, 1, 1, 0, 1, 3, 3, 0, 3, 1, 0, 1, 1, 2, 2, 0, 0, 3, 2, 1, 1, 2, 1, 3, 0, 1, 2, 1, 1, 1, 2, 0, 0, 1, 1, 0, 1, 1, 0, 3], [113278, 74, 86943, 1153, 98323, 7045, 29052, 17, 78844, 900, 249, 31, 3020, 17, 82172, 128486, 74, 148, 82231, 642, 23797, 10665, 52, 17, 2479, 74, 94, 222, 45105, 3065, 4808, 1, 249, 70, 291, 869, 83847, 1, 479, 94, 31, 42181, 62267, 74, 5751, 2, 6649, 6519, 81580, 454, 6596, 259, 105, 46773, 17679, 5125, 624, 216, 2650, 80452, 59235, 120093, 4898, 30340, 9, 74, 1515, 54991, 3658, 77385, 28561, 111201, 41732, 8538, 1897, 167, 102294, 57674, 12433, 11888, 932, 1911, 54054, 113684, 86744, 74, 79527, 3, 1567, 148, 94, 27621, 10, 41420, 71683, 71112, 15749, 931, 12, 10, 73, 580, 269, 102806, 6815, 1542, 40803, 452, 4358, 134, 1, 37173, 25671, 214, 61200, 1, 6545, 9, 66, 63711, 46296, 15594, 37572, 117332, 17469, 196, 8885, 6742], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 2, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 1, 0, 4, 5, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0], [0, 7, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 5, 1, 2, 2, 3, 2, 0, 1, 7, 1, 0, 5, 7, 3, 0, 0, 1, 2, 2, 2, 2, 2, 7, 2, 2, 2, 2, 0, 3, 1, 2, 0, 2, 2, 2, 5, 1, 2, 0, 2, 1, 0, 8, 2, 0, 1, 1, 5, 2, 0, 2, 2, 0, 2, 7, 2, 0, 2, 2, 2, 2, 0, 2, 2, 1, 3, 2, 2, 0, 2, 7, 1, 2, 0, 2, 0, 0, 3, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 7, 1, 1, 3, 0, 11, 2, 1, 1, 2, 2, 0, 0, 0, 0, 0, 1, 0, 3, 0, 2, 2, 2, 2, 1, 0], [329, 4, 440, 733, 1684, 3363, 9694, 8, 19197, 208, 170, 3, 287, 8, 5882, 26577, 4, 105, 3748, 437, 12, 4670, 40, 8, 287, 4, 4, 152, 62, 1705, 18, 1, 170, 9, 196, 12, 49, 1, 122, 4, 3, 1807, 122, 4, 2860, 2, 63, 122, 2881, 305, 12, 175, 77, 129, 649, 2593, 36, 38, 1509, 135, 1397, 5560, 2502, 9997, 8, 4, 18, 569, 1988, 18969, 4, 407, 480, 687, 253, 117, 781, 122, 12, 12, 16, 1145, 5377, 24593, 2954, 4, 13050, 3, 970, 105, 4, 2743, 9, 4233, 1341, 1749, 1589, 611, 11, 9, 4, 391, 181, 49, 3282, 465, 12231, 303, 2259, 77, 1, 3, 12, 36, 16132, 1, 1554, 8, 49, 2131, 504, 2379, 369, 24021, 390, 135, 49, 3261], [1, 1, 16, 5, 1, 1, 7, 1, 5, 9, 9, 0, 1, 1, 1, 2, 1, 1, 1, 1, 10, 11, 6, 1, 1, 1, 1, 2, 1, 5, 1, 22, 9, 1, 6, 11, 5, 1, 1, 1, 0, 1, 1, 1, 0, 0, 5, 1, 1, 1, 5, 5, 1, 0, 1, 6, 1, 1, 1, 1, 5, 14, 0, 1, 1, 1, 1, 11, 1, 1, 1, 1, 5, 1, 5, 5, 7, 1, 9, 12, 2, 5, 1, 11, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 12, 14, 5, 1, 5, 1, 1, 2, 11, 11, 5, 0, 2, 1, 1, 6, 1, 0, 15, 1, 2, 1, 1, 1, 5, 7, 0, 1, 1, 16, 1, 1, 2, 5], [1, 1, 333, 553, 1, 1, 115, 1, 101, 171, 914, 6, 1, 1, 1, 12, 1, 1, 1, 1, 20, 3583, 35, 1, 1, 1, 1, 2157, 1, 14, 1, 1610, 9890, 1, 14, 442, 52, 1, 1, 1, 43, 1, 1, 1, 2176, 629, 59, 1, 1, 1, 20, 207, 1, 105, 1, 2515, 1, 1, 1, 1, 2245, 3986, 1889, 1, 1, 1, 1, 437, 1, 1, 1, 1, 1129, 1, 59, 100, 585, 1, 20, 20, 484, 378, 1, 18531, 1, 1, 1, 3, 1, 1, 1, 2078, 1, 3268, 1007, 1327, 1203, 1, 8, 1, 1, 302, 19, 52, 857, 358, 19, 1, 1, 81, 1, 307, 2058, 1, 166, 1, 1, 1, 47, 1612, 3000, 1, 1, 18103, 1, 1, 52, 2441]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "### parse command line arguments ###\n",
    "#litez: essentially this is the embedding_dim;\n",
    "# may want a better name fot it\n",
    "m_spa = args.arch_sparse_feature_size\n",
    "\n",
    "# litez: here number of dense features is 1\n",
    "num_fea = ln_emb.size + 1  # num sparse + num dense features\n",
    "\n",
    "# litez: do we want to make sure that  m_spa=m_den_out?\n",
    "# otherwise, there may be problems when we are doing feature interaction\n",
    "m_den_out = ln_bot[ln_bot.size - 1]\n",
    "if args.arch_interaction_op == \"dot\":\n",
    "    # approach 1: all\n",
    "    # num_int = num_fea * num_fea + m_den_out\n",
    "    # approach 2: unique\n",
    "    if args.arch_interaction_itself:\n",
    "        num_int = (num_fea * (num_fea + 1)) // 2 + m_den_out\n",
    "    else:\n",
    "        num_int = (num_fea * (num_fea - 1)) // 2 + m_den_out\n",
    "elif args.arch_interaction_op == \"cat\":\n",
    "    num_int = num_fea * m_den_out\n",
    "else:\n",
    "    sys.exit(\n",
    "        \"ERROR: --arch-interaction-op=\"\n",
    "        + args.arch_interaction_op\n",
    "        + \" is not supported\"\n",
    "    )\n",
    "\n",
    "# litez: actually, it is kind of conter-inuitive to adjust the top-layer number\n",
    "# as the definition is not consistent to bottom mlp; we can probably specify \n",
    "# str(num_ini) as an input; of course then, we need to be careful on choosing `dot`\n",
    "# or `cat` operator for feature interactions. One related question is `cat` ever used \n",
    "# in practice?\n",
    "arch_mlp_top_adjusted = str(num_int) + \"-\" + args.arch_mlp_top\n",
    "ln_top = np.fromstring(arch_mlp_top_adjusted, dtype=int, sep=\"-\")\n",
    "\n",
    "# sanity check: feature sizes and mlp dimensions must match\n",
    "if m_den != ln_bot[0]:\n",
    "    sys.exit(\n",
    "        \"ERROR: arch-dense-feature-size \"\n",
    "        + str(m_den)\n",
    "        + \" does not match first dim of bottom mlp \"\n",
    "        + str(ln_bot[0])\n",
    "    )\n",
    "if args.qr_flag:\n",
    "    if args.qr_operation == \"concat\" and 2 * m_spa != m_den_out:\n",
    "        sys.exit(\n",
    "            \"ERROR: 2 arch-sparse-feature-size \"\n",
    "            + str(2 * m_spa)\n",
    "            + \" does not match last dim of bottom mlp \"\n",
    "            + str(m_den_out)\n",
    "            + \" (note that the last dim of bottom mlp must be 2x the embedding dim)\"\n",
    "        )\n",
    "    if args.qr_operation != \"concat\" and m_spa != m_den_out:\n",
    "        sys.exit(\n",
    "            \"ERROR: arch-sparse-feature-size \"\n",
    "            + str(m_spa)\n",
    "            + \" does not match last dim of bottom mlp \"\n",
    "            + str(m_den_out)\n",
    "        )\n",
    "else:\n",
    "    if m_spa != m_den_out:\n",
    "        sys.exit(\n",
    "            \"ERROR: arch-sparse-feature-size \"\n",
    "            + str(m_spa)\n",
    "            + \" does not match last dim of bottom mlp \"\n",
    "            + str(m_den_out)\n",
    "        )\n",
    "if num_int != ln_top[0]:\n",
    "    sys.exit(\n",
    "        \"ERROR: # of feature interactions \"\n",
    "        + str(num_int)\n",
    "        + \" does not match first dimension of top mlp \"\n",
    "        + str(ln_top[0])\n",
    "    )\n",
    "\n",
    "# assign mixed dimensions if applicable\n",
    "if args.md_flag:\n",
    "    m_spa = md_solver(\n",
    "        torch.tensor(ln_emb),\n",
    "        args.md_temperature,  # alpha\n",
    "        d0=m_spa,\n",
    "        round_dim=args.md_round_dims\n",
    "    ).tolist()\n",
    "\n",
    "# test prints (model arch)\n",
    "if args.debug_mode:\n",
    "    print(\"model arch:\")\n",
    "    print(\n",
    "        \"mlp top arch \"\n",
    "        + str(ln_top.size - 1)\n",
    "        + \" layers, with input to output dimensions:\"\n",
    "    )\n",
    "    print(ln_top)\n",
    "    print(\"# of interactions\")\n",
    "    print(num_int)\n",
    "    print(\n",
    "        \"mlp bot arch \"\n",
    "        + str(ln_bot.size - 1)\n",
    "        + \" layers, with input to output dimensions:\"\n",
    "    )\n",
    "    print(ln_bot)\n",
    "    print(\"# of features (sparse and dense)\")\n",
    "    print(num_fea)\n",
    "    print(\"dense feature size\")\n",
    "    print(m_den)\n",
    "    print(\"sparse feature size\")\n",
    "    print(m_spa)\n",
    "    print(\n",
    "        \"# of embeddings (= # of sparse features) \"\n",
    "        + str(ln_emb.size)\n",
    "        + \", with dimensions \"\n",
    "        + str(m_spa)\n",
    "        + \"x:\"\n",
    "    )\n",
    "    print(ln_emb)\n",
    "\n",
    "    print(\"data (inputs and targets):\")\n",
    "    for j, (X, lS_o, lS_i, T) in enumerate(train_ld):\n",
    "        # early exit if nbatches was set by the user and has been exceeded\n",
    "        if nbatches > 0 and j >= 2:\n",
    "            break\n",
    "\n",
    "        print(\"mini-batch: %d\" % j)\n",
    "        print(X.detach().cpu().numpy())\n",
    "        # transform offsets to lengths when printing\n",
    "        print(\n",
    "            [\n",
    "                np.diff(\n",
    "                    S_o.detach().cpu().tolist() + list(lS_i[i].shape)\n",
    "                ).tolist()\n",
    "                for i, S_o in enumerate(lS_o)\n",
    "            ]\n",
    "        )\n",
    "        print([S_i.detach().cpu().tolist() for S_i in lS_i])\n",
    "        print(T.detach().cpu().numpy())\n",
    "\n",
    "ndevices = min(ngpus, args.mini_batch_size, num_fea - 1) if use_gpu else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, sparse_index_group_batch in enumerate(lS_i):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j, (X, lS_o, lS_i, T) in enumerate(train_ld):\n",
    "#     if j==0:\n",
    "#         print(X.detach().cpu().numpy().shape)\n",
    "#         print('len lS_o', len(lS_o))\n",
    "#         for idx, S_o in enumerate(lS_o):\n",
    "#             print('len of single S_o', S_o.shape)\n",
    "#             if idx ==0:\n",
    "#                 print('example single S_o', S_o)\n",
    "#         print('len lS_i', len(lS_i))\n",
    "#         for idx, S_i in enumerate(lS_i):\n",
    "#             print('len of signle S_i', S_i.shape)\n",
    "#             if idx ==0:\n",
    "#                 print('example single S_i', S_i)\n",
    "#         print('len T', len(T))\n",
    "\n",
    "# print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  6,  2, 14,  2, 15,  2,  2,  0,  0,  0,  2], dtype=int32),\n",
       " array([0.000e+00, 3.600e+01, 5.745e+03, 4.056e+03, 0.000e+00, 0.000e+00,\n",
       "        9.700e+01, 1.000e+00, 0.000e+00, 4.770e+02, 9.000e+01, 5.249e+03,\n",
       "        8.700e+01, 1.000e+00, 5.400e+01, 4.702e+03, 0.000e+00, 4.500e+01,\n",
       "        0.000e+00, 3.000e+00, 4.994e+03, 1.000e+00, 7.000e+00, 1.200e+01,\n",
       "        1.100e+01, 2.000e+01]),\n",
       " 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# litez: train_data looks wired...\n",
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial parameters (weights and bias):\n",
      "[[ 0.0151   0.03055 -0.02602 ...  0.00402 -0.02836  0.00095]\n",
      " [-0.0047   0.02269 -0.03016 ...  0.01263 -0.00026 -0.00398]\n",
      " [ 0.02784 -0.01681  0.00642 ... -0.00195  0.02449  0.01874]\n",
      " ...\n",
      " [-0.011    0.01685  0.00188 ...  0.00729 -0.00099  0.02803]\n",
      " [-0.00966  0.02676 -0.01924 ...  0.02036  0.02827  0.0112 ]\n",
      " [ 0.00147 -0.00756  0.00132 ... -0.02577  0.00206  0.02213]]\n",
      "[[-0.03911  0.01796  0.01742 ... -0.00433  0.02673 -0.01448]\n",
      " [ 0.00036 -0.01477  0.03319 ... -0.03843  0.01802  0.00049]\n",
      " [-0.0102  -0.01917  0.03319 ...  0.002   -0.02244 -0.00371]\n",
      " ...\n",
      " [ 0.00834  0.01661 -0.0043  ... -0.00746  0.00449  0.0363 ]\n",
      " [ 0.01619 -0.03124  0.0121  ...  0.02685  0.04001  0.03847]\n",
      " [-0.03661 -0.03387 -0.01511 ...  0.01051 -0.0212  -0.04137]]\n",
      "[[ 2.47746e-05  3.27817e-04 -8.39343e-04 ...  1.02215e-04 -2.27285e-03\n",
      "  -1.74064e-03]\n",
      " [ 2.00169e-04 -2.11335e-03  6.27219e-04 ...  1.01414e-03  5.08524e-04\n",
      "  -7.84518e-04]\n",
      " [ 9.34161e-04  2.02637e-03 -1.25288e-03 ... -3.24764e-04 -1.11631e-03\n",
      "  -2.10314e-03]\n",
      " ...\n",
      " [ 9.08023e-04  1.43634e-03  2.10924e-03 ... -1.91754e-03 -1.66014e-03\n",
      "   1.63651e-03]\n",
      " [-1.28160e-03  1.02813e-03 -3.81607e-05 ... -1.91108e-05  1.86272e-03\n",
      "  -1.71099e-03]\n",
      " [-1.05777e-04 -7.79338e-04  2.10519e-03 ...  4.91117e-04  4.83244e-05\n",
      "   7.39172e-04]]\n",
      "[[ 3.49808e-03 -6.96298e-04 -3.53239e-03 ... -1.19193e-03 -2.84462e-03\n",
      "  -3.50071e-03]\n",
      " [ 2.22069e-03 -3.29650e-03 -1.74141e-03 ... -4.36767e-04  3.48385e-03\n",
      "   3.18088e-03]\n",
      " [-1.49849e-03 -3.94327e-04 -1.66829e-03 ... -2.24740e-03  3.41557e-03\n",
      "  -8.47563e-04]\n",
      " ...\n",
      " [-1.12108e-03  3.29232e-03  1.04266e-03 ...  3.40708e-03 -4.21917e-04\n",
      "  -3.08930e-03]\n",
      " [-3.25510e-03 -2.25443e-03  1.18836e-03 ... -1.16933e-03 -1.73030e-03\n",
      "   3.08862e-03]\n",
      " [-9.41876e-04 -6.03391e-04 -2.57197e-03 ...  1.13908e-03  5.70146e-05\n",
      "   2.63806e-03]]\n",
      "[[-0.06421 -0.00895  0.0334  ... -0.05416  0.0153   0.06084]\n",
      " [ 0.03707 -0.02236 -0.04586 ...  0.04276 -0.02809 -0.01182]\n",
      " [-0.02964  0.01343 -0.05367 ... -0.06431  0.00387  0.04007]\n",
      " ...\n",
      " [ 0.05213 -0.04107  0.01659 ...  0.05814 -0.04427 -0.00645]\n",
      " [ 0.04069 -0.00463 -0.00305 ...  0.00052 -0.04706 -0.01527]\n",
      " [ 0.03273  0.03251 -0.00478 ... -0.03292  0.06342 -0.06098]]\n",
      "[[ 2.35617e-01 -1.03815e-01  9.84890e-02 -4.30096e-02  2.50672e-01\n",
      "   4.78332e-02 -1.75031e-02  1.83704e-01 -8.31934e-02 -2.14510e-01\n",
      "   3.75269e-02 -1.30384e-03  3.22004e-02  2.63578e-01  1.15988e-01\n",
      "  -2.37750e-01]\n",
      " [ 8.73060e-02  1.24587e-01 -1.66533e-01  1.05159e-01 -6.08122e-02\n",
      "   4.47843e-02 -2.82506e-02 -2.06276e-01  2.51679e-02 -1.80733e-01\n",
      "   2.42966e-01 -2.08690e-01 -2.01448e-01 -2.62684e-01  2.48678e-01\n",
      "  -2.90470e-02]\n",
      " [ 1.26968e-01 -2.22925e-01 -4.95223e-02 -2.06627e-01 -1.89902e-01\n",
      "   2.12787e-03 -1.39842e-01 -1.14673e-01  1.82655e-01  2.17656e-01\n",
      "  -1.21289e-01  1.78464e-01  7.77365e-02  2.21631e-02  1.70107e-01\n",
      "   1.73914e-01]\n",
      " [ 1.40666e-01  2.56751e-01 -1.06291e-01  3.46427e-02 -1.94172e-01\n",
      "  -1.53248e-01 -1.66318e-01  2.51102e-01 -2.32692e-01  1.33360e-01\n",
      "   1.37151e-01 -2.24632e-01 -1.02196e-01 -1.29944e-01 -2.17594e-01\n",
      "  -1.78764e-01]\n",
      " [ 1.11620e-02  1.40978e-01  2.03712e-01  1.62800e-01  8.38305e-02\n",
      "  -2.42017e-01  2.14390e-01 -7.31163e-02 -1.46302e-01  1.51479e-01\n",
      "  -1.53839e-01 -1.54776e-02  6.78805e-02  2.56079e-01 -3.70855e-02\n",
      "   2.12975e-01]\n",
      " [-1.47283e-01  1.68722e-01  1.82202e-01  2.26828e-01  1.99936e-02\n",
      "  -2.35515e-03  2.60822e-01  6.99633e-02 -2.22175e-01 -5.93919e-02\n",
      "   1.03379e-01 -2.13992e-01 -2.77341e-02 -1.02813e-01 -2.51947e-01\n",
      "   1.13705e-01]\n",
      " [ 1.79452e-01 -7.82923e-02 -9.72962e-02 -3.11997e-03 -2.44724e-01\n",
      "  -6.16305e-02  1.82428e-01  3.23227e-02  1.71760e-01 -5.16660e-02\n",
      "   1.70944e-04  8.41633e-02  1.06304e-01  2.39613e-01 -1.31360e-01\n",
      "  -2.62785e-01]\n",
      " [ 1.89656e-01 -2.57808e-01 -2.39501e-01  8.82141e-02 -1.84430e-01\n",
      "   2.32998e-02 -1.91478e-02 -2.41465e-02 -2.23951e-01 -5.62793e-03\n",
      "  -2.11921e-01 -1.92117e-01  3.79831e-02 -1.97775e-01  6.99616e-02\n",
      "   6.14080e-02]\n",
      " [-1.97554e-01 -2.20832e-01  2.65345e-01 -1.74132e-01 -2.95365e-02\n",
      "   1.13522e-01 -1.72525e-01 -1.28086e-01 -1.88944e-01  1.42716e-01\n",
      "  -1.62578e-01 -1.78145e-01  1.07447e-01 -1.42525e-02  4.86706e-02\n",
      "  -1.75072e-01]\n",
      " [ 1.84188e-01 -2.22928e-01  2.16552e-01  4.79087e-02 -1.32250e-01\n",
      "   8.65815e-03  2.43003e-01 -1.10061e-01  5.92818e-02  7.47806e-02\n",
      "  -4.83925e-02  1.73353e-01 -8.13264e-02  1.20094e-02 -3.62803e-02\n",
      "  -8.78369e-02]\n",
      " [ 1.49316e-02  7.02730e-02 -1.04748e-02  1.28458e-01  2.52052e-01\n",
      "   1.35159e-02  2.03606e-01 -2.19174e-01 -2.12579e-02  7.74798e-02\n",
      "   2.31090e-01  2.56228e-01  1.69073e-01 -1.40911e-01 -9.55242e-02\n",
      "  -1.20081e-01]\n",
      " [ 2.66420e-01 -1.39101e-01 -1.41208e-01 -1.84072e-01  1.80116e-01\n",
      "   2.44544e-02  5.19477e-02  1.24669e-01 -2.40328e-01 -1.09531e-01\n",
      "   1.64692e-01  3.71500e-02  6.66559e-02 -1.69944e-01  8.25463e-03\n",
      "   2.59872e-01]\n",
      " [ 1.23057e-01  1.45383e-01 -1.35306e-01 -1.73276e-01  5.54838e-02\n",
      "  -2.59602e-03  9.78894e-03 -1.42067e-02 -1.72887e-01  1.83427e-02\n",
      "   1.96188e-01 -2.61067e-01 -1.72203e-01 -2.57297e-01 -1.35878e-01\n",
      "   1.50652e-01]\n",
      " [-2.10182e-01  1.97429e-01 -2.05929e-01  2.08509e-01  2.57324e-01\n",
      "  -1.38324e-01  2.56394e-01  1.91788e-01 -6.33091e-02 -3.53946e-02\n",
      "  -2.03435e-01  1.70070e-01 -9.46188e-02  3.49957e-02  8.41449e-02\n",
      "  -1.75754e-01]]\n",
      "[[ 0.00211 -0.00683 -0.00321 ...  0.00976  0.00662  0.00605]\n",
      " [-0.00892 -0.00521 -0.00546 ... -0.002    0.00246  0.0081 ]\n",
      " [-0.00382 -0.00699  0.00634 ... -0.00904 -0.00616  0.00893]\n",
      " ...\n",
      " [ 0.00659  0.00298  0.0074  ... -0.00331  0.00432 -0.00498]\n",
      " [ 0.00419 -0.00928 -0.00357 ... -0.0031  -0.00926  0.00075]\n",
      " [-0.00661 -0.00096  0.00732 ... -0.0053   0.00342  0.00744]]\n",
      "[[-0.00524 -0.03622  0.03538 ...  0.00718  0.03581  0.01623]\n",
      " [-0.00943 -0.005   -0.02664 ... -0.00786 -0.03662 -0.0213 ]\n",
      " [-0.04594  0.01843 -0.02786 ...  0.00848 -0.01963  0.02668]\n",
      " ...\n",
      " [ 0.01649  0.0064   0.03821 ...  0.03133 -0.01331 -0.0157 ]\n",
      " [ 0.03661  0.00216 -0.03797 ...  0.02881 -0.00364 -0.03526]\n",
      " [ 0.00359 -0.01919 -0.03728 ... -0.01798  0.01369 -0.04405]]\n",
      "[[ 0.5631  -0.10411 -0.31784  0.257   -0.40574 -0.026   -0.52764 -0.42897\n",
      "   0.47814 -0.07651 -0.24346 -0.41589  0.12934 -0.08106  0.08557 -0.11353]\n",
      " [ 0.29467 -0.07886 -0.50976  0.30513 -0.24091 -0.28278  0.50899  0.35198\n",
      "   0.09706 -0.07264  0.03008 -0.03233  0.52648  0.36658 -0.32954 -0.40241]\n",
      " [-0.38011  0.38733 -0.08484  0.5335  -0.27412  0.46625  0.35243 -0.42719\n",
      "  -0.10838 -0.46572 -0.41192  0.57208 -0.14263 -0.44239  0.31302 -0.34036]]\n",
      "[[ 2.40051e-04  5.40207e-03  1.84757e-03 ...  3.20585e-03  4.98085e-03\n",
      "  -6.49938e-03]\n",
      " [-6.62006e-04  2.26885e-03  1.06926e-03 ... -5.98396e-03  1.15911e-03\n",
      "   5.62898e-03]\n",
      " [ 1.90391e-03  2.08930e-03 -2.04411e-03 ...  6.90557e-04 -1.04578e-04\n",
      "  -2.62681e-03]\n",
      " ...\n",
      " [ 2.38109e-03  4.26219e-03  1.11116e-03 ... -3.68674e-03  3.06153e-03\n",
      "  -1.15167e-03]\n",
      " [ 1.27114e-03  3.50236e-03 -8.77401e-05 ... -3.06154e-03 -1.12397e-04\n",
      "  -1.79524e-03]\n",
      " [-1.21148e-03 -1.97171e-03  5.35506e-04 ...  5.75489e-03  2.26812e-03\n",
      "  -5.80151e-04]]\n",
      "[[-0.01     0.01439  0.00503 ...  0.00116 -0.00536  0.01046]\n",
      " [-0.00067  0.0024   0.00957 ... -0.01354  0.00173  0.01028]\n",
      " [ 0.01062 -0.00866  0.00637 ...  0.0017   0.00829 -0.00072]\n",
      " ...\n",
      " [-0.00743 -0.00439  0.0077  ...  0.00183  0.00738  0.00859]\n",
      " [-0.00968  0.00778 -0.00953 ... -0.0086   0.00445 -0.00524]\n",
      " [ 0.00651 -0.00831 -0.01233 ...  0.01376 -0.00072 -0.01252]]\n",
      "[[ 1.72171e-03 -2.05151e-03 -9.39976e-04 ... -9.04549e-04 -2.07743e-03\n",
      "  -2.40646e-03]\n",
      " [ 3.72204e-04  8.42728e-04  9.64546e-04 ... -2.93882e-04  2.37839e-03\n",
      "  -3.62584e-04]\n",
      " [ 2.02578e-03  1.14196e-03 -2.46509e-04 ... -5.98337e-05  1.19365e-03\n",
      "  -2.32822e-03]\n",
      " ...\n",
      " [ 1.71476e-03  2.21906e-03 -2.26544e-03 ...  2.20407e-03  8.39825e-04\n",
      "   1.59402e-03]\n",
      " [ 9.50530e-04  7.23166e-04 -1.12508e-03 ... -4.67316e-04  2.11847e-03\n",
      "  -1.62124e-03]\n",
      " [ 2.07963e-03  6.96929e-04 -9.57663e-04 ... -1.06866e-03 -1.78597e-03\n",
      "   7.84465e-04]]\n",
      "[[-0.01025 -0.00467  0.01412 ... -0.01519 -0.00039 -0.00748]\n",
      " [ 0.01532  0.00602 -0.01626 ... -0.00011  0.01599  0.00773]\n",
      " [-0.0091   0.01707  0.00731 ... -0.0136  -0.01177 -0.01589]\n",
      " ...\n",
      " [-0.00367  0.01331 -0.00256 ... -0.01     0.01331  0.01642]\n",
      " [-0.0078  -0.01475 -0.00932 ...  0.00905 -0.01081  0.00525]\n",
      " [ 0.01689 -0.00655 -0.01568 ...  0.00741  0.00726  0.01753]]\n",
      "[[ 0.17577  0.07585  0.13637  0.14665 -0.03294 -0.0947  -0.18317 -0.06179\n",
      "  -0.02394  0.10414  0.16453 -0.16258 -0.16786 -0.01881  0.13266 -0.10421]\n",
      " [ 0.15368 -0.0458   0.05445  0.11102  0.04329 -0.13867 -0.12451 -0.13538\n",
      "   0.19017  0.17041 -0.05952 -0.12678 -0.0141   0.15603  0.00866  0.14492]\n",
      " [ 0.12632  0.11079  0.04711 -0.06555  0.14375  0.18732 -0.0584   0.15513\n",
      "   0.04386  0.08969  0.01916 -0.19377  0.11861  0.18057  0.01252 -0.04882]\n",
      " [ 0.07098  0.1711  -0.06743 -0.15454 -0.17791 -0.04028  0.03039 -0.11071\n",
      "  -0.12352  0.08801  0.05242 -0.09036  0.14457  0.15919  0.07151  0.19595]\n",
      " [-0.10531 -0.01441  0.12082  0.04077 -0.19171 -0.01376  0.00878 -0.16179\n",
      "  -0.15541 -0.09823 -0.07113  0.18116 -0.19167  0.12514 -0.07532  0.14823]\n",
      " [ 0.0728  -0.05263 -0.17389 -0.19491  0.10631 -0.13463  0.01931  0.09613\n",
      "  -0.16751 -0.0092   0.14652 -0.18416 -0.10965  0.12145  0.01292 -0.02527]\n",
      " [ 0.08854 -0.06206 -0.00991  0.19597 -0.18157  0.05227 -0.15601  0.03189\n",
      "   0.10812 -0.09764  0.1885  -0.19476 -0.12494  0.04688 -0.11503  0.01873]\n",
      " [-0.00715  0.02807 -0.09483  0.11112 -0.06933 -0.07909  0.00752  0.14647\n",
      "  -0.05665 -0.07051  0.15482  0.11481  0.10668  0.06687  0.10335  0.15201]\n",
      " [-0.11708  0.18716  0.13708 -0.03339 -0.10636 -0.16625  0.00021 -0.17853\n",
      "   0.10011  0.07165 -0.14131  0.1144   0.0751  -0.16199  0.1752  -0.07002]\n",
      " [ 0.0499  -0.12965  0.12847 -0.10592  0.12709 -0.10595  0.11124  0.18467\n",
      "   0.04805  0.10844 -0.00602 -0.18337  0.00066  0.08101 -0.08722  0.00419]\n",
      " [-0.01425 -0.0181   0.10575 -0.12743  0.00307 -0.01899  0.07665  0.09207\n",
      "  -0.16932 -0.02695  0.12897  0.15066 -0.01717  0.05266 -0.13568  0.12356]\n",
      " [-0.08961 -0.13815 -0.15009 -0.10195  0.18293 -0.16     0.10311  0.16033\n",
      "  -0.17418  0.13708 -0.10651  0.12676  0.09228 -0.06022 -0.12352  0.16889]\n",
      " [ 0.19519  0.1462   0.18776  0.14148  0.13015 -0.0415   0.0551  -0.16502\n",
      "  -0.16119 -0.06936 -0.06049  0.15564  0.08095  0.05661 -0.04707 -0.1424 ]\n",
      " [-0.10914 -0.15374 -0.00997 -0.18836 -0.16622 -0.01843  0.18895 -0.06192\n",
      "  -0.0939  -0.13084  0.09453  0.08502  0.03211  0.1953   0.00107 -0.19596]\n",
      " [ 0.07808 -0.05419 -0.06704  0.16698  0.03303  0.17718 -0.1398   0.06119\n",
      "  -0.01555  0.17548  0.08397  0.14673  0.12395  0.08659 -0.02903 -0.09609]\n",
      " [-0.06236 -0.1449   0.01945  0.14578  0.17347  0.03669  0.06511 -0.03069\n",
      "  -0.03473 -0.06066 -0.14354 -0.14576 -0.03928  0.07419  0.10628  0.06186]\n",
      " [ 0.0585   0.04408 -0.00441  0.07472 -0.09128  0.17624 -0.00578  0.10449\n",
      "  -0.01242 -0.11335  0.06431 -0.13381 -0.10364  0.07163  0.18843  0.06319]\n",
      " [-0.16817 -0.013   -0.12223 -0.08805 -0.08717 -0.13423 -0.15605 -0.04145\n",
      "  -0.17154  0.19529  0.12444  0.1282  -0.08521  0.08861  0.04054  0.13499]\n",
      " [ 0.03557 -0.0955  -0.03758 -0.09506 -0.05669  0.13164 -0.12412  0.13449\n",
      "   0.154    0.11088  0.00898  0.02717  0.00471 -0.09246 -0.03608  0.12695]\n",
      " [-0.14781  0.07747  0.09298 -0.15178 -0.16919  0.11271  0.01032  0.16855\n",
      "   0.09338  0.08089 -0.15482  0.11134  0.1597  -0.0433   0.04324  0.14672]\n",
      " [ 0.11108  0.03105  0.07299 -0.15642 -0.0159  -0.09584  0.02336 -0.0387\n",
      "  -0.00529  0.05921  0.18972 -0.01829  0.17895 -0.11006  0.08686  0.15562]\n",
      " [ 0.13588 -0.05819 -0.13553  0.14533 -0.14861 -0.07739 -0.00682 -0.00136\n",
      "   0.06375 -0.18663 -0.17664  0.18316 -0.06003  0.18577  0.15397  0.01506]\n",
      " [-0.177    0.01283  0.03878  0.13043  0.05206  0.19016  0.0148   0.15607\n",
      "  -0.00895 -0.12825  0.05289  0.12058 -0.0526  -0.19089 -0.03851 -0.19422]\n",
      " [ 0.14178  0.07233  0.09062 -0.09429 -0.1934  -0.0698  -0.11381 -0.18833\n",
      "  -0.0658  -0.1441  -0.13956  0.03363 -0.00814 -0.16058  0.09537  0.00509]\n",
      " [ 0.06738  0.0316  -0.01553  0.12608 -0.07956 -0.04269  0.13964 -0.17344\n",
      "   0.11857  0.1827   0.15281  0.17301 -0.12762 -0.15305 -0.02376  0.12131]\n",
      " [-0.15746  0.14755 -0.04635  0.03693 -0.16326 -0.16241 -0.19007  0.14704\n",
      "   0.1811  -0.18381 -0.18058 -0.00379 -0.04462  0.1368  -0.13533 -0.04848]]\n",
      "[[ 0.00606 -0.00412 -0.00262 ... -0.00814 -0.00656 -0.00967]\n",
      " [-0.0036  -0.00244 -0.00552 ...  0.00637  0.00461 -0.00431]\n",
      " [ 0.01084  0.00443 -0.00766 ... -0.00323  0.00064  0.009  ]\n",
      " ...\n",
      " [ 0.00814  0.0037   0.00087 ...  0.00785 -0.00354  0.01102]\n",
      " [ 0.0021   0.00268  0.0047  ... -0.00861  0.00333  0.00579]\n",
      " [-0.00828  0.00875  0.00129 ... -0.0002   0.00876 -0.00192]]\n",
      "[[-0.00065  0.00095 -0.00174 ... -0.00188  0.00247 -0.00154]\n",
      " [ 0.00145 -0.00015  0.00219 ... -0.00187  0.00145 -0.0026 ]\n",
      " [ 0.00164 -0.00137  0.00254 ... -0.00111  0.00178 -0.00242]\n",
      " ...\n",
      " [-0.00203 -0.00169  0.00183 ... -0.00034  0.00012  0.00093]\n",
      " [-0.00272  0.00064 -0.00208 ... -0.00033  0.00242 -0.0017 ]\n",
      " [ 0.00271  0.00111  0.00026 ...  0.00129 -0.00169 -0.00267]]\n",
      "[[-0.10371 -0.29116  0.21083  0.12716  0.27882 -0.04444 -0.06897  0.1212\n",
      "   0.21247 -0.17889  0.17573 -0.05258  0.13694  0.06501  0.07681  0.14554]\n",
      " [ 0.30581  0.08284 -0.12805 -0.1306  -0.14297  0.10656 -0.14856 -0.02307\n",
      "  -0.10277  0.04799 -0.04606  0.25008 -0.22689  0.29961  0.00051  0.29769]\n",
      " [-0.23577  0.0095  -0.02193 -0.25164 -0.04717  0.28152  0.02029 -0.06152\n",
      "  -0.0117  -0.1014   0.28191  0.21552  0.16748  0.01003  0.23631 -0.13979]\n",
      " [ 0.04422 -0.04475 -0.22916 -0.24102 -0.23983  0.27802  0.30577 -0.14388\n",
      "   0.06358 -0.27325 -0.09608  0.19049 -0.28847  0.27428 -0.21661  0.00733]\n",
      " [-0.20582  0.17481 -0.19875 -0.0483  -0.18919 -0.04692 -0.15536 -0.30545\n",
      "  -0.09674 -0.16323  0.26858 -0.24159  0.08563  0.29358  0.31491  0.25442]\n",
      " [-0.14959  0.24753 -0.29119 -0.12585  0.03209  0.0452   0.25081  0.23854\n",
      "  -0.1113   0.16013 -0.26181  0.29127  0.0455   0.22055  0.08887  0.26915]\n",
      " [-0.02521  0.03949  0.21503 -0.16091  0.26063  0.09706 -0.04878 -0.25209\n",
      "   0.29128 -0.29848  0.00868  0.18212  0.25181  0.23327 -0.29829  0.2636 ]\n",
      " [ 0.15519  0.20402 -0.03593  0.27918  0.02331  0.01051  0.0884   0.13535\n",
      "   0.30148  0.11827  0.0198   0.04291  0.23732 -0.2399  -0.01085  0.23913]\n",
      " [-0.19765 -0.1906  -0.26121 -0.00177 -0.26049 -0.28914 -0.13419  0.01622\n",
      "  -0.29346  0.11388 -0.26428  0.17762  0.21648  0.05397 -0.26885  0.06727]\n",
      " [ 0.04044  0.11935 -0.16524  0.24728 -0.19156 -0.28534 -0.18854 -0.2502\n",
      "  -0.18948  0.26341 -0.03082  0.25051  0.04579 -0.13437  0.08285  0.16637]]\n",
      "[[-0.0038   0.01194  0.0039  ... -0.00175  0.01291 -0.00226]\n",
      " [ 0.00812 -0.01092 -0.01012 ...  0.0028   0.00295  0.00343]\n",
      " [-0.00048 -0.01635 -0.01528 ... -0.00305  0.01166 -0.00802]\n",
      " ...\n",
      " [-0.01181  0.00686  0.00683 ... -0.01372 -0.00181  0.00824]\n",
      " [-0.00823  0.00135 -0.00184 ...  0.00767  0.00163 -0.00774]\n",
      " [ 0.0136  -0.00656  0.00628 ...  0.0154   0.01473  0.00963]]\n",
      "[[ 0.01197 -0.02001 -0.01683 ... -0.02031  0.00519 -0.01103]\n",
      " [-0.00574  0.00382  0.00694 ...  0.01269  0.01549  0.0011 ]\n",
      " [ 0.01254 -0.02309 -0.01113 ...  0.00805  0.0095  -0.01634]\n",
      " ...\n",
      " [-0.01127 -0.02293  0.01507 ...  0.02398 -0.00314  0.02355]\n",
      " [ 0.00059 -0.00539 -0.0206  ...  0.00604 -0.01678 -0.0188 ]\n",
      " [-0.01184 -0.02123 -0.01706 ... -0.00865 -0.01165  0.02341]]\n",
      "[[-0.20402 -0.05351  0.02661  0.49258 -0.31182  0.03138  0.47161  0.2599\n",
      "   0.01471  0.30267  0.11374 -0.08442  0.17037  0.18272  0.00646  0.4375 ]\n",
      " [ 0.13009  0.3908   0.48194 -0.31774 -0.10523  0.00276 -0.46307 -0.19999\n",
      "  -0.423   -0.20316  0.26167  0.36052  0.49627 -0.30038  0.1804   0.13634]\n",
      " [ 0.43223  0.09488  0.25764 -0.03016 -0.13244  0.2355  -0.29878 -0.19045\n",
      "  -0.45438  0.33601  0.09619  0.18374 -0.39667 -0.16339 -0.46858  0.01092]\n",
      " [ 0.10239 -0.43845  0.18238  0.48266 -0.09574  0.01321  0.01983 -0.19128\n",
      "  -0.1291   0.46007 -0.22252 -0.1758   0.04502 -0.29528 -0.04877 -0.02864]]\n",
      "[[-2.17773e-03  1.66033e-03 -1.51908e-03 ...  2.37807e-03  1.66854e-03\n",
      "   3.29138e-04]\n",
      " [-2.14744e-03  1.69563e-03 -2.17807e-03 ... -1.45130e-03  1.93939e-03\n",
      "  -1.46885e-03]\n",
      " [-1.25904e-03  8.65171e-04  2.49130e-03 ... -1.62424e-03 -2.08623e-03\n",
      "  -1.27542e-03]\n",
      " ...\n",
      " [-5.95249e-04 -1.58407e-03  1.33461e-03 ...  1.37473e-03  2.57373e-03\n",
      "   1.16436e-03]\n",
      " [ 2.59030e-04  2.14527e-03 -1.00331e-03 ... -2.03319e-05 -1.83316e-03\n",
      "  -5.38665e-04]\n",
      " [ 1.33767e-03 -1.74755e-03 -2.20299e-03 ... -2.21982e-03  1.38810e-03\n",
      "   1.03145e-03]]\n",
      "[[ 0.16543  0.08768  0.23638  0.21578 -0.11783 -0.03543  0.22209  0.09756\n",
      "  -0.12481  0.22814 -0.10169  0.01103  0.1448  -0.09756  0.16071 -0.02566]\n",
      " [-0.05123  0.25516  0.22367  0.25307 -0.07272  0.11608 -0.07622 -0.09801\n",
      "  -0.12237  0.15946 -0.12276  0.25175 -0.21237 -0.11697 -0.05533  0.12947]\n",
      " [-0.09555 -0.21595  0.08547  0.09359 -0.16898  0.0236  -0.16927 -0.12983\n",
      "  -0.06651  0.14265 -0.11909 -0.00157  0.09305 -0.19818  0.18089 -0.08546]\n",
      " [ 0.16627 -0.18685 -0.07879 -0.15675 -0.17301  0.2072  -0.03724 -0.17978\n",
      "  -0.15043 -0.17749  0.09291  0.15604 -0.00178 -0.08425 -0.07366  0.16184]\n",
      " [ 0.02956  0.15934 -0.04795  0.17457  0.09842 -0.15444 -0.04379 -0.23624\n",
      "   0.01831  0.15839 -0.024   -0.12536 -0.14737 -0.15232 -0.2557  -0.02187]\n",
      " [ 0.18254 -0.08625 -0.07939 -0.03838  0.06476 -0.20016  0.05603  0.02368\n",
      "  -0.03508 -0.12765  0.16671  0.01458 -0.21173 -0.25508 -0.24876  0.13989]\n",
      " [-0.20127  0.02628  0.05755  0.09542 -0.1532  -0.11755  0.16887  0.13213\n",
      "   0.02376  0.04831 -0.08547 -0.03451 -0.04724  0.01409 -0.05282  0.06364]\n",
      " [-0.01817 -0.24093 -0.09894 -0.21315  0.13662 -0.03794 -0.0356   0.06767\n",
      "   0.21006  0.08101 -0.06509 -0.08647  0.10126  0.16697 -0.22471  0.24279]\n",
      " [ 0.24421  0.11072  0.25273  0.0753   0.23767 -0.15936 -0.10286 -0.20225\n",
      "   0.17353  0.12326 -0.18745  0.02282 -0.18123 -0.22771 -0.24484 -0.24835]\n",
      " [-0.01065 -0.04257  0.04779 -0.00459 -0.03188 -0.08336 -0.18966 -0.12474\n",
      "  -0.24319 -0.05622  0.15978 -0.20748 -0.01219 -0.06574  0.06491 -0.21535]\n",
      " [-0.08963  0.13315  0.16571  0.21154 -0.06173 -0.21457 -0.22242 -0.03955\n",
      "   0.11322 -0.0581   0.23275 -0.12618 -0.18021  0.121    0.03517  0.22188]\n",
      " [-0.11088  0.08532 -0.20887 -0.20505  0.16603  0.24574 -0.11484  0.06613\n",
      "  -0.05814 -0.0583  -0.12842  0.23424 -0.20561  0.16175  0.00891 -0.19781]\n",
      " [-0.16269  0.10718 -0.12454 -0.23524  0.0617  -0.14315 -0.21039  0.06683\n",
      "   0.16987  0.0166  -0.15259  0.15144 -0.02776 -0.10751 -0.16793 -0.17769]\n",
      " [-0.14028 -0.0206  -0.11215  0.24298  0.1034   0.00484  0.02764  0.11943\n",
      "  -0.16357  0.00978  0.11307  0.06434 -0.2578  -0.25378 -0.02505  0.08021]\n",
      " [ 0.24765 -0.19964  0.0776  -0.17049  0.10322  0.19315  0.20413  0.07078\n",
      "   0.01134 -0.12041  0.10317 -0.06842 -0.25374 -0.01069  0.17306  0.0613 ]]\n",
      "[[ 0.25763 -0.19961 -0.02947  0.20069  0.11366 -0.08472  0.26477 -0.00919\n",
      "   0.10714 -0.17942 -0.2318   0.05112 -0.1558   0.0299   0.17083 -0.17639]\n",
      " [ 0.01633  0.13206  0.04333  0.04657 -0.05954 -0.04029 -0.11698 -0.09754\n",
      "   0.04923  0.15552  0.21275 -0.26293 -0.03928  0.23606  0.12496 -0.13506]\n",
      " [ 0.22503 -0.18381 -0.14633  0.16904  0.23083  0.04415  0.09711  0.1237\n",
      "  -0.14884  0.18592 -0.12235  0.25827  0.14608 -0.23973  0.26683  0.20154]\n",
      " [ 0.10467 -0.19584  0.02235  0.02513 -0.10879  0.04921  0.21704 -0.17585\n",
      "   0.03868 -0.01028  0.05596  0.11416 -0.01421  0.19826  0.23125  0.06756]\n",
      " [-0.21024  0.0238  -0.11597  0.00954  0.17292 -0.00433 -0.03777 -0.05532\n",
      "  -0.21725 -0.01307 -0.26337 -0.26329  0.25781  0.02492  0.25434 -0.16599]\n",
      " [ 0.17757  0.16109 -0.26528  0.00859 -0.25226  0.00775  0.15817 -0.01747\n",
      "  -0.11205  0.23793 -0.02847 -0.25559  0.20148 -0.07823 -0.06965  0.09542]\n",
      " [ 0.24852  0.0785  -0.06346 -0.16081  0.00203 -0.19505  0.25482 -0.03822\n",
      "   0.06619 -0.0303   0.22296 -0.15697  0.10813  0.10903  0.0431   0.05218]\n",
      " [ 0.05138 -0.07726 -0.09237  0.04402 -0.14663 -0.06242  0.19533  0.22492\n",
      "  -0.20159 -0.10994 -0.14662 -0.23832 -0.23486  0.1808   0.05149 -0.09076]\n",
      " [ 0.0099   0.19004 -0.1446   0.22561 -0.1506   0.03854  0.08639  0.02535\n",
      "  -0.18555 -0.17974 -0.01094 -0.19319  0.03611  0.08907 -0.17895 -0.25635]\n",
      " [ 0.05988 -0.13074 -0.17289  0.10325  0.11393 -0.15367  0.04269 -0.14178\n",
      "  -0.20874 -0.22716 -0.01091 -0.21756  0.08681  0.12255 -0.04798  0.16911]\n",
      " [-0.23874  0.03181  0.0474  -0.16691  0.21741  0.24902 -0.23595  0.22933\n",
      "   0.25905  0.26476  0.18348  0.08958  0.12376  0.00073  0.19315  0.02125]\n",
      " [-0.19945 -0.22391  0.13469 -0.24377  0.03186  0.0936   0.02298  0.18135\n",
      "  -0.13513 -0.18117 -0.11302 -0.15638  0.20476 -0.0411   0.11988 -0.07398]\n",
      " [ 0.226   -0.02544  0.15312  0.12249  0.25902  0.09758  0.04976 -0.16047\n",
      "   0.07466 -0.17995 -0.23789 -0.1075   0.08693  0.01901 -0.2486   0.11752]\n",
      " [ 0.03551  0.0554   0.14391  0.1603  -0.20023  0.18281 -0.14217 -0.0393\n",
      "   0.07656 -0.06273  0.05941  0.16761 -0.07588 -0.22604  0.19407 -0.10437]]\n",
      "[[ 3.28800e-03  2.36164e-03 -5.37497e-03 ... -1.94260e-03 -3.11312e-04\n",
      "   9.67356e-04]\n",
      " [ 2.32864e-03  1.65105e-05  3.17120e-03 ...  3.41931e-03 -3.50478e-03\n",
      "  -4.97503e-03]\n",
      " [-2.04173e-03  3.64041e-03 -3.90312e-03 ...  5.51400e-03  1.58903e-03\n",
      "  -2.51941e-03]\n",
      " ...\n",
      " [-4.12997e-03 -5.75180e-03 -1.36201e-03 ... -5.42428e-03 -4.55363e-03\n",
      "   5.76484e-03]\n",
      " [-4.79614e-03  4.09250e-03  1.83277e-03 ... -2.12528e-03 -2.31247e-03\n",
      "   4.81018e-03]\n",
      " [ 4.67570e-03  5.32826e-03  3.70169e-03 ...  2.77619e-03  2.49746e-03\n",
      "  -2.78111e-03]]\n",
      "[[ 0.06175  0.01205 -0.07883 ... -0.10111 -0.06219 -0.07335]\n",
      " [-0.1003  -0.01701  0.11799 ...  0.07209  0.0285   0.02181]\n",
      " [ 0.09258  0.09331  0.09381 ...  0.08087 -0.11815 -0.08863]\n",
      " ...\n",
      " [ 0.07405 -0.11604 -0.04371 ... -0.12032  0.00447 -0.10192]\n",
      " [-0.02755  0.02986  0.01969 ... -0.08325 -0.04362 -0.11981]\n",
      " [-0.00497 -0.08847 -0.09001 ... -0.0805   0.0977   0.071  ]]\n",
      "[[ 0.00084 -0.00137  0.0031  ...  0.00356  0.00579 -0.0028 ]\n",
      " [-0.00247 -0.00334  0.00045 ... -0.00582 -0.0014   0.00143]\n",
      " [ 0.00094 -0.00349  0.00589 ... -0.00126 -0.00088 -0.00512]\n",
      " ...\n",
      " [-0.00274  0.00193  0.00065 ... -0.00116  0.00384  0.00419]\n",
      " [ 0.00147  0.0048   0.00229 ...  0.0033   0.00211 -0.0008 ]\n",
      " [ 0.00416 -0.00268 -0.00194 ... -0.00497 -0.00217 -0.00623]]\n",
      "[[-0.03132 -0.0067  -0.05295 ... -0.11001  0.02665  0.00528]\n",
      " [ 0.029   -0.10805 -0.11117 ... -0.00524  0.04239 -0.03569]\n",
      " [-0.13548 -0.02927 -0.0738  ... -0.04964  0.01478  0.14779]\n",
      " ...\n",
      " [-0.0283  -0.14001  0.02719 ...  0.02569  0.03448  0.09343]\n",
      " [ 0.03305  0.07793 -0.0785  ...  0.07799  0.07211 -0.07227]\n",
      " [-0.05267 -0.01263  0.08374 ... -0.01064  0.02425  0.03498]]\n",
      "[ 1.80401e-02  4.00485e-02  2.41496e-02  5.53370e-02  4.65864e-02\n",
      "  1.57343e-02 -4.11601e-02  2.72413e-02 -3.27477e-02 -3.89230e-02\n",
      "  3.39488e-03  6.28536e-02 -1.42030e-02  3.84579e-02 -3.56118e-02\n",
      " -2.40083e-02 -2.46979e-02  1.57005e-02  2.09790e-02 -3.20607e-03\n",
      " -5.60494e-03 -5.17233e-03  1.97615e-02 -5.51065e-02 -1.23601e-02\n",
      " -3.02832e-02 -1.44778e-02  8.36987e-02 -2.88087e-02  4.35889e-02\n",
      " -2.90996e-02 -1.58329e-02 -6.42670e-03 -2.22452e-02  1.21216e-04\n",
      " -7.81323e-03 -2.65200e-02  2.28084e-03 -5.54917e-03  8.18324e-02\n",
      " -2.07025e-02 -3.75073e-03  2.98793e-02  1.43074e-02 -1.11539e-02\n",
      " -2.37102e-02  4.30757e-03  2.94186e-02 -6.84942e-02  5.33194e-03\n",
      " -1.48688e-02 -6.02845e-02  1.32657e-02  3.66360e-02  8.54615e-03\n",
      " -7.55049e-03 -6.99711e-02 -1.76879e-02 -3.64711e-02 -4.71219e-02\n",
      " -2.80159e-02  1.00057e-01  5.84496e-03  6.67616e-02 -9.66712e-03\n",
      " -1.22468e-02 -5.32052e-02  6.44680e-02 -2.96992e-02  3.70644e-03\n",
      "  4.86935e-02  2.44461e-03 -2.34729e-02  5.40972e-02  2.04379e-02\n",
      "  8.27588e-02  4.99346e-02  1.83870e-02 -8.19071e-02  6.98414e-02\n",
      "  3.65296e-02 -1.85458e-02  3.82315e-02 -4.46101e-02 -7.35128e-02\n",
      "  6.34692e-02  1.41439e-02 -1.77202e-02  1.69402e-02 -2.20776e-02\n",
      "  2.38368e-02 -3.81395e-02  7.75559e-03  1.02032e-02  8.83022e-03\n",
      "  8.32909e-02  5.90674e-02 -1.56010e-02 -4.60261e-02 -4.35385e-03\n",
      "  2.01819e-02 -2.45868e-02 -5.11241e-02  5.58557e-02 -1.12344e-02\n",
      "  4.41977e-03 -7.37993e-02 -5.33629e-02  4.84423e-02  1.20554e-02\n",
      " -2.60865e-02 -2.77565e-02 -5.66600e-02 -2.72335e-02 -1.35692e-01\n",
      "  3.44463e-02 -2.62050e-02 -1.41247e-02  3.72313e-02  2.24569e-02\n",
      "  5.06892e-02  6.06459e-02  1.86232e-02  4.21179e-02  1.18324e-02\n",
      "  2.59226e-02 -5.16744e-02 -4.54679e-02 -9.83014e-03 -5.21918e-02\n",
      " -2.06980e-02 -2.17448e-02 -5.65663e-02 -3.46113e-02  6.44270e-02\n",
      " -1.33484e-02 -6.98952e-02  5.44551e-02 -4.43575e-02  3.79881e-02\n",
      " -2.43236e-02 -8.63364e-02 -2.48486e-02  1.86818e-02 -7.69809e-02\n",
      " -5.34771e-02 -2.21669e-02 -7.91914e-03 -7.01356e-02 -1.39129e-02\n",
      " -6.89453e-02  3.33440e-02  5.46596e-03  3.89000e-02 -1.70630e-03\n",
      "  3.38417e-02 -1.60407e-02 -6.34193e-03 -2.66243e-02 -4.28586e-02\n",
      "  2.01012e-02 -3.06999e-03 -1.45239e-03 -1.18116e-02 -7.06849e-03\n",
      "  2.29494e-02  3.98944e-02 -3.19305e-03  5.62685e-03  1.80762e-02\n",
      "  1.17986e-02 -3.13027e-02 -1.77895e-02  2.70414e-02 -5.38584e-02\n",
      "  2.67511e-02 -1.32811e-02 -1.41630e-02  5.60811e-02 -2.57709e-02\n",
      " -5.03074e-02 -3.62200e-02  5.42920e-02  4.18766e-03  1.45185e-03\n",
      "  6.32568e-02 -8.27553e-02 -1.43385e-03 -4.87414e-02 -1.39481e-02\n",
      "  1.67000e-02 -5.25177e-02 -3.46879e-02 -3.52811e-02 -9.15400e-03\n",
      "  3.33003e-02 -3.58801e-02  6.28681e-03  4.36470e-02 -6.01988e-02\n",
      "  7.25902e-02 -3.54857e-02  4.56144e-02 -5.42514e-02  5.63864e-02\n",
      " -1.86897e-02  1.13520e-02  2.36083e-03 -3.47392e-02  2.99027e-02\n",
      " -1.82311e-03  1.73812e-02  1.30205e-03 -4.43062e-04  3.08392e-03\n",
      "  1.01224e-03  7.90277e-02  5.30484e-02 -5.01135e-02  7.21778e-02\n",
      " -9.09552e-02 -4.62447e-02 -3.66000e-02 -1.18212e-01 -2.79493e-02\n",
      " -3.95077e-02 -8.65366e-02 -2.61073e-02  1.68402e-02  1.49676e-02\n",
      " -2.66585e-02 -2.63624e-02 -6.38523e-03 -1.44153e-03 -1.02870e-02\n",
      "  6.39829e-02  3.65436e-02  2.55890e-02  9.78281e-02  6.03034e-02\n",
      "  7.43912e-02  4.40839e-03 -1.70091e-02  5.90128e-02 -6.60975e-02\n",
      "  4.16865e-02 -3.23812e-03  5.17042e-02  5.61551e-03  1.85855e-02\n",
      "  1.31543e-02  9.14297e-03  1.44757e-02 -2.66601e-02  5.88334e-02\n",
      "  7.57254e-02 -2.81700e-02  8.53544e-04 -8.20624e-02  3.41456e-02\n",
      "  2.95435e-02  2.02810e-02 -5.77447e-02 -9.25661e-02  1.35865e-03\n",
      "  2.74425e-02  1.48198e-02 -2.72657e-02 -3.41836e-02  4.19970e-02\n",
      "  5.07293e-02  2.44944e-03 -2.55310e-02 -2.42888e-03  2.66980e-02\n",
      " -1.40983e-02 -4.83938e-02 -3.33142e-02 -4.85350e-02 -1.71813e-02\n",
      " -8.49869e-02  7.75908e-02  3.10908e-03  1.66078e-03  3.35341e-02\n",
      " -2.11538e-03 -6.05029e-03 -4.77941e-02 -2.91340e-02  8.62061e-02\n",
      "  6.14751e-02  3.37139e-03  9.43411e-03  6.29565e-04 -3.13484e-02\n",
      " -5.08106e-02  6.71045e-04  5.55049e-03  1.11718e-01 -4.06818e-02\n",
      "  7.74932e-02 -1.27822e-02 -3.09723e-02  1.08121e-02  1.30584e-02\n",
      " -2.95656e-02 -8.55269e-03  2.27427e-02  1.33869e-02 -1.04427e-02\n",
      "  7.40171e-02 -3.10883e-02  1.73554e-02 -3.33737e-02  1.84799e-02\n",
      "  3.61104e-02  4.09202e-02 -2.20716e-02 -4.85364e-02  1.12493e-01\n",
      "  2.07078e-02  2.35614e-02  5.07387e-02  7.59014e-02 -6.42703e-02\n",
      "  5.74297e-03  5.22802e-02 -3.02588e-02 -7.61024e-03 -3.62958e-02\n",
      "  4.09507e-03  4.87690e-03 -2.30457e-02 -3.75305e-03 -9.49735e-03\n",
      " -4.36244e-02  1.15431e-02 -2.07479e-03 -4.23156e-02  9.91493e-02\n",
      "  1.73526e-02  2.31999e-03 -3.55405e-02  7.78934e-02 -2.49974e-02\n",
      " -3.89154e-02 -3.70978e-02 -8.31662e-02 -5.84831e-03 -2.18413e-02\n",
      " -8.89753e-02  1.76200e-02  3.41060e-02  1.93490e-03  5.05229e-02\n",
      "  1.50836e-02 -6.94652e-02  6.85802e-02  9.00399e-05  2.79949e-02\n",
      "  3.44746e-02 -1.91969e-02 -6.08156e-02 -3.13043e-02  5.02490e-02\n",
      " -1.59243e-03  1.38172e-02 -3.31927e-02 -8.29789e-03 -7.29995e-02\n",
      "  3.11656e-02 -1.24163e-02 -7.67752e-03 -1.52948e-02 -6.68951e-02\n",
      " -1.31433e-02 -5.32800e-02  5.18894e-02 -1.12725e-02  6.01259e-02\n",
      "  1.87770e-02  2.00581e-02  2.67165e-02 -3.50117e-02  3.66034e-02\n",
      " -2.54020e-02 -3.95501e-02  2.44687e-02  9.43126e-03 -3.87533e-03\n",
      " -9.26163e-02 -5.27298e-02 -1.69740e-02  1.52732e-02 -3.97161e-02\n",
      "  3.39067e-02 -1.94273e-02  2.18593e-02 -2.29431e-02 -2.85672e-02\n",
      " -1.99435e-02  2.61830e-02  4.19483e-03 -6.19166e-02  3.12985e-02\n",
      "  3.51918e-02  2.37645e-02 -6.26627e-02  5.78001e-02 -1.82214e-02\n",
      " -1.70006e-02  8.16291e-03  2.02763e-02 -3.55299e-02  1.73541e-02\n",
      " -3.79315e-03  1.08436e-02 -9.26349e-03  1.07276e-01  5.68806e-02\n",
      " -2.50575e-02 -3.45341e-02  1.29375e-02  3.24869e-02 -2.57523e-02\n",
      " -4.61986e-02  3.79244e-03 -3.66753e-02  1.26096e-02  2.75062e-02\n",
      " -1.81883e-02  2.85599e-02  2.01638e-02 -1.56029e-02 -1.22111e-02\n",
      " -2.97830e-02  5.76379e-03 -2.05805e-02  2.55807e-02 -6.15123e-02\n",
      " -1.09346e-02 -4.81287e-02 -8.32992e-02  2.12893e-02 -2.37670e-02\n",
      "  8.55977e-02 -9.43971e-02 -5.41368e-02 -1.00540e-01  4.08646e-02\n",
      " -4.98195e-02  8.62080e-03  1.03589e-01  1.53171e-02 -8.10922e-03\n",
      " -2.02142e-02  1.73456e-02  3.75744e-02 -1.50015e-02  6.01672e-02\n",
      " -3.33617e-02 -5.41905e-02  3.29504e-02 -2.15511e-02  1.61387e-02\n",
      "  7.50334e-02 -1.64385e-02 -2.03487e-03  1.63737e-02  6.36507e-02\n",
      "  2.32593e-02  2.74170e-02  4.41390e-02  3.39192e-02 -1.60499e-03\n",
      "  5.42095e-02 -4.12952e-02  3.67277e-02  4.95552e-03 -4.62136e-02\n",
      " -1.36932e-02  5.08104e-02 -1.24328e-01  1.26187e-02 -7.40260e-02\n",
      "  4.44213e-02  3.71037e-02 -5.58881e-02 -3.80233e-02  2.28813e-02\n",
      " -4.26735e-02  1.57213e-02  7.01152e-02 -6.20134e-02 -5.60771e-02\n",
      " -6.55126e-02  1.53500e-02  3.49591e-02  1.66190e-02 -1.77642e-02\n",
      " -1.86080e-02  1.65045e-03 -6.96787e-02 -3.88681e-02  6.73347e-03\n",
      "  8.14112e-02  1.93954e-02 -7.24525e-02  7.69950e-03 -7.74306e-02\n",
      " -1.82990e-02 -4.33243e-02]\n",
      "[[-0.00197 -0.04218 -0.01098 ... -0.01613  0.0369   0.02247]\n",
      " [ 0.04159  0.11548 -0.02947 ... -0.00876 -0.10603 -0.00708]\n",
      " [-0.11238  0.00714 -0.0857  ... -0.00371  0.07269  0.01438]\n",
      " ...\n",
      " [-0.03498 -0.00333 -0.10099 ... -0.01206 -0.01297 -0.00768]\n",
      " [-0.04783 -0.08298  0.00778 ...  0.02153 -0.03137  0.03124]\n",
      " [-0.03045  0.01315  0.00971 ... -0.06674  0.01854  0.0017 ]]\n",
      "[-0.02187  0.06354 -0.08799 -0.08316 -0.04653  0.09808 -0.02049 -0.16648\n",
      " -0.01301  0.05172 -0.05754  0.09687  0.0254  -0.03127 -0.01489 -0.08603\n",
      " -0.04856 -0.12704 -0.01288 -0.01725 -0.06177  0.00053  0.00357 -0.09991\n",
      " -0.09349  0.0634   0.01261 -0.00253  0.13067 -0.03254  0.09782 -0.01044\n",
      " -0.00815 -0.04987  0.0884  -0.04684 -0.03995 -0.0236   0.00798 -0.09642\n",
      " -0.06164 -0.01306  0.01226  0.0359   0.00518 -0.01935  0.01712  0.02287\n",
      "  0.00478  0.00863  0.05317  0.02537 -0.10007  0.12274 -0.06878 -0.09327\n",
      " -0.05675  0.10898  0.05546 -0.13481 -0.03437 -0.08988 -0.07619 -0.08664\n",
      "  0.03115  0.09801 -0.03837 -0.12982 -0.01708  0.06502  0.06281  0.01999\n",
      " -0.09966 -0.05042  0.08768 -0.0832  -0.01849 -0.02015  0.06191 -0.0446\n",
      " -0.03614  0.00444 -0.03965 -0.08098  0.04677 -0.06654  0.07126 -0.08961\n",
      "  0.0114  -0.066    0.08493 -0.01812 -0.09734 -0.02145  0.04426  0.11024\n",
      " -0.00961  0.12513  0.00085 -0.01865  0.06333 -0.06812 -0.02927 -0.02584\n",
      " -0.00158 -0.10886  0.01455  0.07759  0.03465 -0.0307   0.15152  0.02974\n",
      "  0.0555  -0.09722 -0.07299  0.0332   0.10001 -0.06907  0.03635 -0.06117\n",
      " -0.02872  0.01155 -0.06481  0.09394  0.03815  0.07361 -0.15131 -0.04336\n",
      "  0.00482  0.00675 -0.12398 -0.08953 -0.09722  0.0293   0.07109  0.08262\n",
      "  0.01002 -0.01154  0.05963  0.00379 -0.05807  0.07454 -0.09763  0.14552\n",
      "  0.0875  -0.08643  0.17251 -0.02382 -0.09733 -0.04225 -0.03012 -0.0107\n",
      " -0.03411 -0.02806 -0.04781  0.11992  0.07686 -0.04766  0.00817  0.02271\n",
      "  0.14224 -0.02633  0.06145  0.00322 -0.03359  0.0529   0.02797  0.03211\n",
      " -0.03286  0.01137 -0.04413 -0.02738 -0.02933  0.05371 -0.05394  0.04589\n",
      " -0.03952  0.00451  0.01436  0.10464 -0.10508  0.0763   0.00573  0.03308\n",
      " -0.06272 -0.01026  0.02125 -0.03771 -0.05991  0.09741 -0.00035  0.04062\n",
      " -0.05442  0.02224  0.04746 -0.06294  0.14423  0.02448  0.05093 -0.08266\n",
      " -0.0299  -0.04101  0.06125  0.05204 -0.03264 -0.07148  0.11826 -0.09081\n",
      "  0.05955  0.02805  0.00082 -0.01621 -0.07077 -0.00964  0.07725 -0.02242\n",
      " -0.07323 -0.0013  -0.03544 -0.01333  0.05577 -0.00381 -0.00648 -0.01015\n",
      " -0.02993  0.02498  0.03346  0.01351  0.02068 -0.04908  0.02151  0.069\n",
      " -0.10879 -0.09736  0.05807  0.02427 -0.05227 -0.0222  -0.00605  0.11713\n",
      "  0.04983  0.05963  0.02477 -0.00295 -0.03765  0.13339  0.0674  -0.0266\n",
      "  0.02146 -0.04532  0.02611  0.07344  0.00054 -0.08944 -0.09307 -0.0462 ]\n",
      "[[-0.04775  0.19413 -0.14949 ...  0.10287 -0.02823 -0.02927]\n",
      " [ 0.11384  0.15639  0.08873 ...  0.02906  0.13012  0.08925]\n",
      " [-0.00778 -0.07957 -0.1176  ...  0.11754  0.046   -0.02413]\n",
      " ...\n",
      " [-0.00142 -0.01217 -0.06062 ...  0.05807 -0.0982   0.14049]\n",
      " [ 0.06788 -0.02632 -0.00047 ...  0.06176 -0.02515 -0.00614]\n",
      " [-0.03754 -0.00205 -0.03321 ... -0.00754  0.00041  0.11793]]\n",
      "[-0.06703 -0.09152 -0.17034  0.09478 -0.08153  0.13358 -0.09442  0.01008\n",
      "  0.11548  0.01455  0.089   -0.04225  0.13757  0.19985 -0.04839 -0.22655\n",
      " -0.1113   0.11907 -0.00343  0.18417 -0.05184 -0.02641 -0.23522 -0.02074\n",
      "  0.01629  0.12854  0.09214  0.01393 -0.10771 -0.10348 -0.06751 -0.06061\n",
      "  0.04095 -0.04297 -0.23776  0.18119  0.10579 -0.08154  0.15633 -0.02857\n",
      " -0.07578 -0.01181  0.0894  -0.05024 -0.06866 -0.04971  0.14048  0.10292\n",
      " -0.0525   0.12105  0.12288  0.18594 -0.03344 -0.06906  0.05911  0.10601\n",
      "  0.19901 -0.03128  0.17444  0.03891 -0.08212  0.03958 -0.11768  0.12045]\n",
      "[[-0.24636 -0.00794  0.18858 ...  0.06542 -0.16004  0.08459]\n",
      " [-0.13811 -0.00179  0.01646 ...  0.14374  0.15171  0.07321]\n",
      " [ 0.14441 -0.09175 -0.03416 ...  0.13282  0.20427 -0.30875]\n",
      " ...\n",
      " [-0.0739  -0.03424  0.07767 ...  0.10755 -0.02133 -0.2723 ]\n",
      " [ 0.31981 -0.07658 -0.0164  ...  0.15805  0.03634 -0.05221]\n",
      " [-0.08556  0.08743  0.08941 ... -0.12295 -0.1173  -0.04854]]\n",
      "[ 0.03022 -0.40559  0.0725  -0.11929  0.26437 -0.13521  0.34321  0.13582\n",
      "  0.02317 -0.36466  0.13819 -0.19783  0.04097 -0.21225 -0.11402  0.07881]\n",
      "[[ 0.05849 -0.00874  0.0287  ... -0.11938  0.0554  -0.08878]\n",
      " [ 0.028    0.01501 -0.04946 ... -0.07417 -0.07284 -0.04211]\n",
      " [ 0.01302 -0.08513  0.00771 ... -0.04802 -0.07173 -0.05248]\n",
      " ...\n",
      " [ 0.03892 -0.02931  0.01291 ... -0.01187 -0.02884  0.03238]\n",
      " [-0.0427   0.04796  0.00421 ... -0.00517  0.0278  -0.00044]\n",
      " [ 0.00917 -0.0333  -0.037   ... -0.02329  0.0069  -0.03053]]\n",
      "[ 3.47385e-02  2.18689e-02 -3.36209e-02  2.09249e-03  7.12652e-02\n",
      "  1.04295e-02 -2.56893e-02  1.18612e-01  1.15773e-02 -1.51284e-02\n",
      " -1.60890e-01 -2.73911e-03  1.55600e-02 -2.38996e-03  6.51548e-02\n",
      "  1.97824e-02  7.67703e-02  5.79848e-02 -1.85458e-02  7.77214e-03\n",
      " -4.70814e-02  5.47927e-02  1.63716e-02 -9.56423e-02 -2.44280e-02\n",
      "  3.42840e-02  3.12163e-02  7.26249e-02  4.58318e-02 -2.75411e-02\n",
      "  1.17694e-01 -2.66828e-02  6.31162e-03  1.60252e-02 -4.43667e-02\n",
      "  6.34717e-02 -2.92818e-02  6.47124e-02  1.98500e-02 -1.62137e-02\n",
      " -5.15341e-02  1.02547e-01  4.44259e-02 -5.03422e-03  6.13772e-02\n",
      "  1.77071e-03 -4.78782e-02  5.06634e-02  1.40354e-02  5.18464e-03\n",
      " -8.08712e-02  1.10230e-02  9.39443e-02 -4.89193e-02 -1.52474e-02\n",
      "  4.26344e-03 -6.00680e-02  4.16646e-04  3.86931e-02  1.04706e-02\n",
      " -2.10597e-02  2.89191e-02  2.32818e-02 -2.94660e-02 -4.17612e-02\n",
      " -1.24098e-01  2.03283e-02  2.49753e-02 -2.54117e-02  7.77068e-02\n",
      " -1.36840e-02 -1.32241e-02  5.78077e-02 -8.97640e-03 -5.01067e-03\n",
      " -2.39758e-02  1.31325e-01 -1.06157e-02 -3.67217e-02  4.25909e-02\n",
      "  1.66505e-02  8.45824e-02 -3.38164e-02  3.55527e-02  9.80262e-02\n",
      "  3.99495e-02 -7.53236e-03  2.25866e-02  6.22511e-03 -1.51780e-02\n",
      "  2.43843e-02  8.46732e-02 -1.92997e-03  2.80182e-03 -1.91617e-02\n",
      "  2.03001e-02  2.02558e-02  1.83588e-02 -8.11971e-03 -6.44113e-03\n",
      " -2.70890e-02  3.62039e-02  1.29844e-02  6.45477e-02 -4.96313e-02\n",
      "  2.20143e-02  4.12185e-02 -4.14474e-02 -1.08296e-03  6.46438e-02\n",
      " -1.66259e-02 -5.82708e-02  1.89452e-02  4.89257e-02 -4.13422e-02\n",
      " -6.37488e-02  1.07103e-02  5.11144e-02  3.23076e-02 -2.09505e-02\n",
      " -2.67794e-02  1.55654e-02 -6.21170e-02  1.13553e-02  6.02106e-02\n",
      "  6.08839e-02  1.07642e-03 -1.34727e-02 -2.39607e-02  3.22179e-02\n",
      " -3.22380e-02 -1.27641e-02 -1.37093e-03 -4.09043e-02  4.06160e-03\n",
      "  3.43461e-03  7.03402e-04 -9.91307e-02  3.19422e-02 -6.25366e-02\n",
      " -3.94248e-02 -7.82383e-02 -8.92781e-04 -8.49068e-03  4.85658e-02\n",
      " -1.48104e-02  3.51302e-02 -1.49291e-02 -4.83616e-02  2.04373e-02\n",
      "  6.23973e-02  2.70428e-03 -6.57502e-02 -2.80880e-02 -1.22707e-01\n",
      "  3.26619e-02 -3.52455e-02  8.16369e-02  3.47380e-02  1.58171e-02\n",
      " -1.91677e-02 -1.38931e-02 -1.39675e-02 -9.20840e-02  2.64681e-02\n",
      " -2.54877e-02 -1.53353e-02 -1.35057e-02 -4.28546e-02  6.00913e-02\n",
      "  8.30053e-02 -1.77327e-02  2.67399e-02  1.15241e-02 -3.66331e-02\n",
      "  3.67138e-02 -2.30229e-02 -9.12982e-03  7.85657e-03  1.81036e-02\n",
      "  3.94553e-03 -1.08314e-02  1.19399e-01 -3.97331e-02  8.25528e-02\n",
      " -5.81431e-02  2.56660e-02  4.43300e-02  3.68917e-03 -2.70525e-02\n",
      "  1.53466e-02 -3.82902e-02 -1.19768e-01  8.79118e-04  6.36447e-03\n",
      "  7.22744e-03 -2.71054e-02  3.44934e-02 -2.33409e-02 -1.58305e-02\n",
      " -2.06453e-02  1.53803e-02 -5.15383e-02 -4.17847e-02 -5.15020e-02\n",
      " -4.04581e-02 -4.25071e-02 -2.46490e-02  3.70505e-02 -2.93649e-02\n",
      "  7.15237e-02 -4.14240e-03 -1.38820e-02 -9.16601e-02 -2.39686e-02\n",
      "  2.66461e-02 -1.01977e-01 -1.07783e-01  4.15804e-02  5.72341e-02\n",
      "  1.04836e-02 -4.57288e-02  3.66483e-02  9.94081e-02  3.00459e-02\n",
      " -7.63467e-02  1.95731e-02 -2.31505e-02 -1.02587e-01 -6.02815e-02\n",
      "  5.70649e-02  5.23605e-02  2.43151e-02 -4.21806e-03  1.08189e-02\n",
      "  6.14778e-03 -4.89397e-02 -3.92210e-02  4.58414e-02  3.35861e-02\n",
      "  8.85010e-02  6.60183e-03  4.34801e-03  3.30601e-02  4.64041e-02\n",
      "  1.05769e-02 -2.59969e-02  2.20769e-02 -1.72284e-03 -1.99285e-02\n",
      " -3.51232e-03  1.59832e-03  2.92554e-02 -1.92409e-02 -1.10686e-02\n",
      " -4.11241e-02  3.70041e-02  3.02319e-02 -2.05515e-02  2.40740e-02\n",
      "  1.26953e-02  5.02263e-02  1.02626e-01  2.44348e-02 -3.83463e-02\n",
      "  1.84298e-02  3.84132e-02 -3.23522e-03  1.65362e-02  3.57271e-02\n",
      " -9.14388e-03  3.32780e-02 -9.86238e-02  4.83735e-02  2.48475e-02\n",
      " -5.00266e-02 -5.67144e-02  1.18161e-03  4.54306e-02  1.63309e-02\n",
      "  1.77986e-02 -5.97896e-03 -1.18279e-02  5.89618e-02 -1.36046e-02\n",
      " -1.08174e-02  7.37054e-03  5.67345e-02 -5.35005e-02  3.36051e-03\n",
      " -1.80303e-02 -6.79598e-03  2.95822e-02  2.73230e-02 -5.57017e-05\n",
      "  3.60938e-02 -1.25734e-02 -3.61621e-02 -6.23815e-03 -1.14587e-02\n",
      "  1.01529e-03 -2.21514e-02  2.87591e-02  6.23242e-02 -8.78252e-02\n",
      " -3.08677e-02 -1.97501e-02  7.30845e-02 -3.38075e-02 -2.64362e-02\n",
      " -2.26899e-02 -5.48044e-03  2.44930e-02  3.41129e-03  8.21510e-02\n",
      " -1.31636e-02 -3.86716e-02 -2.41002e-02 -7.87874e-03 -2.48021e-02\n",
      "  6.92398e-02  2.35502e-02  3.07122e-02 -3.23290e-02  2.46144e-02\n",
      "  5.81914e-02 -2.30983e-02 -5.08165e-03 -6.99706e-02 -5.30700e-02\n",
      "  3.27016e-02 -1.46038e-02  3.19635e-02 -2.47090e-02 -3.16727e-02\n",
      "  6.63074e-02 -2.54903e-02 -1.87007e-02 -2.05330e-02  1.75686e-02\n",
      " -8.06658e-02 -6.78461e-02  1.15297e-02 -1.97287e-02 -1.02209e-02\n",
      "  5.48800e-02  4.63319e-02  3.59990e-02 -1.43025e-02  7.91089e-02\n",
      " -7.51704e-02 -2.81503e-02  9.66793e-03  2.59537e-02  4.09890e-02\n",
      "  3.95321e-02 -7.99991e-03 -2.70587e-02  3.47700e-02  8.08155e-02\n",
      "  4.55939e-02 -2.54318e-02  5.77760e-02  1.01969e-01  1.99135e-02\n",
      "  5.60301e-02 -1.51222e-02  3.87798e-02  4.57552e-02  8.19765e-03\n",
      " -5.31833e-02 -4.29000e-02  2.62391e-02  5.59335e-03 -2.84737e-02\n",
      " -4.25566e-02 -4.80206e-02  4.84621e-02  7.10502e-02 -1.00216e-02\n",
      "  7.67629e-02 -3.83533e-02  2.13662e-02  1.16259e-01  4.66887e-02\n",
      " -2.38362e-02  3.13000e-02  4.11183e-02  1.75628e-02 -1.73968e-02\n",
      "  5.16834e-02 -4.91947e-02  1.92059e-02  1.23736e-02 -3.93525e-02\n",
      "  3.89611e-02 -3.80202e-02  9.53958e-02 -1.29633e-02  1.57333e-02\n",
      " -2.06991e-02 -1.89017e-02  1.73576e-02  2.81518e-02 -1.63020e-03\n",
      " -6.35140e-02  8.28352e-02 -1.44276e-02  1.10660e-02  9.30658e-05\n",
      " -6.42332e-02  2.13778e-02  1.07598e-01 -9.46099e-02 -8.11753e-03\n",
      " -1.22964e-02  4.05701e-03 -3.30221e-02  2.31354e-02 -6.11434e-02\n",
      " -3.81892e-02  1.46456e-03 -2.27636e-02 -1.71046e-02  1.50585e-02\n",
      "  4.99321e-02 -5.44638e-02 -9.04221e-03  9.42021e-02 -1.14510e-02\n",
      " -1.31401e-02 -3.39991e-02  2.53243e-02 -4.49638e-02 -3.23666e-02\n",
      " -3.94353e-02 -8.78038e-02  1.23875e-02 -8.58154e-03 -2.00640e-02\n",
      "  3.52308e-02  3.39147e-02 -3.23596e-02 -4.93869e-02 -1.54414e-02\n",
      " -4.39105e-02 -1.90568e-02 -4.95224e-02 -1.28897e-02 -9.59836e-02\n",
      "  2.76095e-02  4.08521e-05 -1.94531e-02  1.20114e-02  1.83828e-02\n",
      " -2.40413e-02 -6.24162e-02  3.33903e-02 -5.58344e-02 -1.20124e-02\n",
      " -2.67010e-03  1.41957e-02  1.73584e-02  2.68326e-03  8.55864e-02\n",
      " -2.22366e-02 -4.45374e-02  2.52481e-02  5.17148e-02 -8.03394e-02\n",
      " -3.49417e-02  4.13471e-02 -2.15005e-03  1.81468e-02  2.85210e-02\n",
      " -3.24396e-02 -7.84600e-03 -1.97267e-02 -6.07678e-02  9.52341e-03\n",
      "  2.58159e-02 -2.59650e-03 -1.15938e-03  8.00004e-03  2.59700e-02\n",
      " -1.34233e-02 -3.43633e-02 -4.77007e-02 -1.34291e-02 -3.30098e-02\n",
      " -6.86008e-02 -4.24098e-04  5.86948e-02 -1.18466e-02 -1.56000e-03\n",
      "  5.65843e-02 -2.02306e-02 -2.22120e-02 -7.52215e-03 -9.97470e-03\n",
      " -3.77570e-02  7.83560e-03  3.50032e-02 -1.86670e-02  3.45872e-02\n",
      " -5.36367e-02  6.10976e-02  8.38819e-03  6.18991e-03  5.81531e-02\n",
      " -3.87356e-02  2.48955e-02]\n",
      "[[-0.00288 -0.00455  0.02545 ... -0.02216 -0.01144 -0.05698]\n",
      " [-0.03334 -0.02713  0.04025 ... -0.02747 -0.03993  0.06174]\n",
      " [-0.01367 -0.02964 -0.0025  ... -0.00869 -0.01667  0.01842]\n",
      " ...\n",
      " [-0.17943 -0.02374 -0.01975 ...  0.02245 -0.07103 -0.06123]\n",
      " [ 0.00027 -0.10038 -0.00526 ...  0.04926  0.01665 -0.05561]\n",
      " [-0.01748 -0.04212 -0.01933 ...  0.01676 -0.00824 -0.00767]]\n",
      "[-2.89373e-02  1.15753e-01 -7.74630e-03  4.52393e-02 -7.06849e-02\n",
      "  4.20667e-02  1.16080e-01  4.70193e-02 -6.96504e-02  1.62321e-02\n",
      " -4.18272e-02 -3.22725e-02  6.17969e-02 -3.94646e-02  1.05621e-01\n",
      "  5.66748e-02 -1.21930e-01 -3.11784e-02  1.13159e-02  3.83662e-02\n",
      " -5.39182e-02 -5.28366e-02 -5.64920e-02 -6.64261e-02  1.36409e-01\n",
      " -4.54220e-02 -6.85444e-02  9.36296e-02  9.25745e-02  9.35919e-02\n",
      " -1.07298e-01 -5.22602e-02 -7.46496e-02 -4.14505e-02  6.18306e-02\n",
      " -1.32711e-02  6.50559e-02 -1.87426e-02  1.64519e-02  6.23331e-02\n",
      " -4.58183e-02 -5.54755e-02 -2.92925e-02 -5.05591e-03 -6.98620e-04\n",
      "  3.78675e-02 -5.58526e-02  4.61284e-02 -5.35554e-02  8.77421e-03\n",
      " -3.11246e-02  9.25627e-02  1.59716e-02  1.10527e-04  1.15307e-02\n",
      " -4.18871e-02  7.82166e-02  3.75131e-02 -1.17592e-01 -4.34743e-02\n",
      "  5.28819e-02 -7.93058e-02 -1.35833e-01  7.48862e-02  4.47659e-02\n",
      "  8.31359e-02 -5.37462e-02 -3.82629e-02  1.40023e-02  1.52333e-03\n",
      " -5.73982e-02 -4.88553e-04 -2.55555e-02 -1.54594e-02  1.81490e-02\n",
      "  8.62201e-02 -1.30626e-02 -5.74072e-02  5.86756e-02 -3.90179e-02\n",
      "  9.80714e-02  6.47846e-02  1.92823e-02  1.04610e-01  9.04698e-02\n",
      "  2.40706e-02  1.01683e-02 -7.54076e-02  8.88159e-02  8.12663e-02\n",
      " -5.88740e-02 -1.05765e-01  1.12731e-01  8.26482e-02  3.12951e-02\n",
      "  5.27366e-02 -5.21271e-02  1.65665e-02  5.07191e-02  5.02035e-02\n",
      " -4.45630e-02 -8.33053e-02  1.11081e-01  3.39741e-02 -1.09423e-01\n",
      " -9.71384e-02 -9.02965e-03  4.04469e-02 -3.23081e-03  8.94165e-03\n",
      "  2.08900e-02  2.87162e-02 -4.07352e-02 -1.01886e-01 -2.31556e-02\n",
      " -7.94131e-02 -4.96871e-02  8.42506e-02 -6.39798e-02  1.16632e-03\n",
      " -7.07520e-03 -2.88450e-02  1.99175e-02 -6.71359e-02 -5.90296e-02\n",
      "  1.06592e-01  2.23468e-02  1.53681e-01  1.35481e-02  1.64498e-01\n",
      "  9.13472e-03  1.63240e-02  2.46520e-03 -7.17520e-03  2.77087e-02\n",
      " -1.11840e-01  1.36607e-02 -7.97765e-02 -4.06890e-02 -3.38485e-02\n",
      " -6.25964e-02 -8.51537e-02  1.64388e-03  1.50485e-02 -7.02997e-02\n",
      "  5.48661e-03  6.67366e-02  2.03025e-02  7.10332e-02  4.14447e-02\n",
      " -3.22399e-02 -2.93968e-02 -2.61577e-02  6.76350e-04 -1.11139e-01\n",
      "  1.83147e-02 -8.50692e-03 -3.89986e-03  2.24003e-02 -2.96949e-02\n",
      "  2.44234e-02 -7.22773e-02  3.35316e-02 -1.22226e-01  5.08117e-02\n",
      "  2.29840e-02  1.09518e-02  2.40936e-02  1.94294e-03  6.78979e-02\n",
      " -3.55156e-02 -1.16616e-02 -5.17227e-02  5.04786e-02 -4.39800e-02\n",
      " -2.37892e-02  2.20836e-01 -6.95129e-02 -2.94363e-02 -4.95083e-02\n",
      "  2.65686e-02  9.19678e-02 -5.82451e-02  4.66969e-02 -4.79558e-02\n",
      " -1.22202e-01 -8.17949e-02  4.68851e-02  6.40463e-02  1.20779e-02\n",
      "  8.90800e-02  1.49521e-02  3.21423e-02  1.14197e-01 -6.74281e-02\n",
      " -4.52439e-02  1.74549e-02  2.75271e-02  7.10407e-04  1.08182e-01\n",
      "  3.19749e-04 -9.91936e-03 -9.00973e-02 -3.93635e-03  3.44573e-04\n",
      " -6.89436e-04  6.00325e-02  1.71136e-02 -1.97392e-02 -9.39903e-02\n",
      " -4.50774e-03  1.15423e-01  3.71833e-02 -1.45878e-02 -2.92647e-02\n",
      "  4.58412e-02 -2.16949e-02  5.09635e-02 -1.12796e-01 -1.37654e-01\n",
      "  1.13618e-01 -5.36538e-02 -7.09061e-02  5.79031e-02  1.43008e-02\n",
      "  8.75530e-02 -6.36091e-02 -3.36269e-02  3.54305e-02 -4.58611e-02\n",
      "  3.62502e-02 -3.20164e-02 -9.59067e-03 -1.85130e-02  1.41593e-01\n",
      " -3.43473e-02 -4.04980e-03 -1.13854e-01 -3.98740e-02  6.22629e-03\n",
      " -3.89746e-03 -1.66039e-02  1.49610e-02 -7.55920e-02 -6.99369e-02\n",
      " -5.43023e-02 -2.66945e-03 -1.69706e-02 -1.21149e-01  3.44381e-02\n",
      " -1.46540e-01  5.26269e-03 -4.69005e-02 -8.24865e-02 -5.07488e-02\n",
      "  9.94109e-03]\n",
      "[[ 4.33140e-02 -1.56594e-02  1.03775e-01 -1.25308e-01 -1.43735e-01\n",
      "   4.79431e-02  7.10098e-02  7.37751e-02  4.39955e-02 -2.87105e-01\n",
      "   9.29240e-04  3.83277e-03  1.31928e-01 -1.58124e-01  9.28756e-02\n",
      "   2.29499e-02  1.93477e-01  6.38394e-02 -1.10151e-01  3.66708e-02\n",
      "   1.88368e-05  2.91848e-03 -1.10693e-02  1.67757e-02  8.58548e-02\n",
      "   5.25148e-02  2.59602e-02 -3.39467e-02 -2.10523e-01  2.26987e-02\n",
      "   5.90337e-02  6.35070e-02 -1.81664e-01  3.38533e-02 -1.01720e-01\n",
      "   1.37560e-01 -9.88482e-02  8.79215e-02  1.40025e-01 -1.90269e-02\n",
      "   1.54573e-01  4.63393e-02  1.47728e-02 -1.86339e-02  3.90173e-02\n",
      "   2.01070e-02  1.52649e-01 -1.20298e-02 -7.42951e-03  2.67753e-02\n",
      "   3.58750e-02  2.96517e-02  3.26386e-02 -1.58599e-01  4.52554e-02\n",
      "   7.32202e-02 -1.10245e-01 -4.47424e-02  1.40635e-02 -2.48675e-02\n",
      "   1.12195e-02 -1.33717e-02  6.17060e-02 -9.85422e-02  7.61048e-02\n",
      "  -7.70687e-02 -6.23041e-02  1.09785e-02 -4.01396e-02  1.55447e-01\n",
      "   7.60263e-02 -4.38370e-02 -1.14168e-02  5.49892e-02  2.82401e-03\n",
      "   1.02879e-01  7.30373e-02 -2.79287e-02 -7.37542e-02 -3.11615e-02\n",
      "   5.16900e-02  8.73744e-02  7.47243e-02 -1.61218e-03  2.91696e-02\n",
      "   6.76151e-02  1.59573e-01  4.24322e-02 -8.89216e-02 -6.82687e-02\n",
      "  -5.67108e-02  8.62807e-02 -5.59714e-02 -1.16195e-01 -1.31910e-01\n",
      "   1.52537e-01  1.11787e-01  1.83995e-01 -9.39734e-02  5.81683e-02\n",
      "   5.77663e-02  3.56550e-02  6.05180e-02 -1.74936e-02 -6.21982e-02\n",
      "  -3.64979e-02  1.35037e-01  6.10075e-02  4.07926e-02 -8.05468e-02\n",
      "   7.50222e-02  8.65355e-02 -5.80152e-02  1.46562e-02 -3.89862e-02\n",
      "  -2.24236e-02  2.85904e-02  1.62687e-01 -1.38424e-01 -1.63580e-01\n",
      "  -2.50184e-02 -5.91907e-02 -3.27325e-02 -2.52506e-03 -2.17504e-02\n",
      "  -6.87477e-02 -9.12182e-03 -7.87407e-02  6.05624e-02  6.20082e-02\n",
      "  -2.47025e-02 -6.06819e-02 -3.51969e-02 -1.49473e-01 -1.21854e-01\n",
      "  -1.19084e-03 -4.27765e-02  1.44567e-01 -2.47882e-02 -7.71595e-02\n",
      "  -2.41191e-01  1.04243e-01 -4.30353e-02 -3.24571e-02  1.06221e-02\n",
      "  -4.85567e-02  1.12227e-03 -2.14223e-01 -3.54426e-02  1.16421e-01\n",
      "   3.94036e-02  2.90505e-02  7.32661e-02  8.16156e-02 -2.01207e-02\n",
      "  -1.47869e-01 -1.49453e-01 -8.64306e-02 -1.04055e-01 -4.47882e-03\n",
      "  -2.50285e-02  2.64843e-02  6.51401e-02 -1.60245e-01 -1.04438e-01\n",
      "   1.34165e-02 -1.15112e-01 -2.94875e-02  8.48890e-02  5.64299e-02\n",
      "   3.26473e-02 -1.44146e-02 -3.55497e-02  4.16518e-02 -1.25739e-01\n",
      "  -1.05290e-02  1.61177e-01  4.81909e-02  2.08122e-02 -9.27442e-02\n",
      "  -9.20304e-03 -1.52791e-03  1.17714e-01 -8.51534e-02  8.97434e-02\n",
      "   2.08577e-02  1.00407e-02  7.17455e-02  4.23913e-02 -4.26143e-02\n",
      "   7.45156e-02  1.94458e-02  1.26765e-02  1.55023e-02  7.85567e-03\n",
      "   5.47788e-02 -1.28180e-02  3.89960e-03 -1.04787e-02  4.92892e-02\n",
      "   9.03576e-02 -1.29872e-03 -1.20327e-01 -1.63301e-01  6.02035e-02\n",
      "   5.04749e-02 -1.14025e-02  6.96136e-02 -1.48309e-02  1.07765e-01\n",
      "   9.85431e-02  1.27428e-02 -4.79106e-02  1.69273e-01 -2.21727e-02\n",
      "  -1.64667e-02 -3.27478e-03  1.89811e-02 -1.21487e-01  3.20027e-02\n",
      "   9.67840e-02  1.16094e-01 -1.03597e-01  1.01193e-03 -1.12624e-01\n",
      "  -1.84967e-01  8.91005e-02 -4.34266e-02  2.79142e-01 -1.72038e-01\n",
      "  -1.58774e-01  1.07433e-01 -3.01483e-03 -9.65115e-02 -1.28225e-01\n",
      "   1.37030e-02  1.83447e-02  7.81226e-02  1.08798e-01 -8.29056e-03\n",
      "  -7.65678e-02 -1.60804e-01  9.27442e-02 -8.03850e-02  2.26552e-01\n",
      "   5.47404e-02 -1.91839e-02  1.05935e-01  1.80642e-01 -9.82939e-02\n",
      "   1.56710e-01  1.03613e-01  6.42160e-02 -4.42875e-02 -1.52192e-01\n",
      "   8.78284e-02]]\n",
      "[1.2923]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "### construct the neural network specified above ###\n",
    "# WARNING: to obtain exactly the same initialization for\n",
    "# the weights we need to start from the same random seed.\n",
    "# np.random.seed(args.numpy_rand_seed)\n",
    "dlrm = DLRM_Net(\n",
    "    m_spa,\n",
    "    ln_emb,\n",
    "    ln_bot,\n",
    "    ln_top,\n",
    "    arch_interaction_op=args.arch_interaction_op,\n",
    "    arch_interaction_itself=args.arch_interaction_itself,\n",
    "    sigmoid_bot=-1,\n",
    "    sigmoid_top=ln_top.size - 2,\n",
    "    sync_dense_params=args.sync_dense_params,\n",
    "    loss_threshold=args.loss_threshold,\n",
    "    ndevices=ndevices,\n",
    "    qr_flag=args.qr_flag,\n",
    "    qr_operation=args.qr_operation,\n",
    "    qr_collisions=args.qr_collisions,\n",
    "    qr_threshold=args.qr_threshold,\n",
    "    md_flag=args.md_flag,\n",
    "    md_threshold=args.md_threshold,\n",
    ")\n",
    "# test prints\n",
    "if args.debug_mode:\n",
    "    print(\"initial parameters (weights and bias):\")\n",
    "    for param in dlrm.parameters():\n",
    "        print(param.detach().cpu().numpy())\n",
    "    # print(dlrm)\n",
    "\n",
    "if use_gpu:\n",
    "    # Custom Model-Data Parallel\n",
    "    # the mlps are replicated and use data parallelism, while\n",
    "    # the embeddings are distributed and use model parallelism\n",
    "    dlrm = dlrm.to(device)  # .cuda()\n",
    "    if dlrm.ndevices > 1:\n",
    "        dlrm.emb_l = dlrm.create_emb(m_spa, ln_emb)\n",
    "\n",
    "# specify the loss function\n",
    "if args.loss_function == \"mse\":\n",
    "    loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "elif args.loss_function == \"bce\":\n",
    "    loss_fn = torch.nn.BCELoss(reduction=\"mean\")\n",
    "elif args.loss_function == \"wbce\":\n",
    "    loss_ws = torch.tensor(np.fromstring(args.loss_weights, dtype=float, sep=\"-\"))\n",
    "    loss_fn = torch.nn.BCELoss(reduction=\"none\")\n",
    "else:\n",
    "    sys.exit(\"ERROR: --loss-function=\" + args.loss_function + \" is not supported\")\n",
    "\n",
    "if not args.inference_only:\n",
    "    # specify the optimizer algorithm\n",
    "    optimizer = torch.optim.SGD(dlrm.parameters(), lr=args.learning_rate)\n",
    "\n",
    "### main loop ###\n",
    "def time_wrap(use_gpu):\n",
    "    if use_gpu:\n",
    "        torch.cuda.synchronize()\n",
    "    return time.time()\n",
    "\n",
    "def dlrm_wrap(X, lS_o, lS_i, use_gpu, device):\n",
    "    if use_gpu:  # .cuda()\n",
    "        # lS_i can be either a list of tensors or a stacked tensor.\n",
    "        # Handle each case below:\n",
    "        lS_i = [S_i.to(device) for S_i in lS_i] if isinstance(lS_i, list) \\\n",
    "            else lS_i.to(device)\n",
    "        lS_o = [S_o.to(device) for S_o in lS_o] if isinstance(lS_o, list) \\\n",
    "            else lS_o.to(device)\n",
    "        return dlrm(\n",
    "            X.to(device),\n",
    "            lS_o,\n",
    "            lS_i\n",
    "        )\n",
    "    else:\n",
    "        return dlrm(X, lS_o, lS_i)\n",
    "\n",
    "def loss_fn_wrap(Z, T, use_gpu, device):\n",
    "    if args.loss_function == \"mse\" or args.loss_function == \"bce\":\n",
    "        if use_gpu:\n",
    "            return loss_fn(Z, T.to(device))\n",
    "        else:\n",
    "            return loss_fn(Z, T)\n",
    "    elif args.loss_function == \"wbce\":\n",
    "        if use_gpu:\n",
    "            loss_ws_ = loss_ws[T.data.view(-1).long()].view_as(T).to(device)\n",
    "            loss_fn_ = loss_fn(Z, T.to(device))\n",
    "        else:\n",
    "            loss_ws_ = loss_ws[T.data.view(-1).long()].view_as(T)\n",
    "            loss_fn_ = loss_fn(Z, T.to(device))\n",
    "        loss_sc_ = loss_ws_ * loss_fn_\n",
    "        # debug prints\n",
    "        # print(loss_ws_)\n",
    "        # print(loss_fn_)\n",
    "        return loss_sc_.mean()\n",
    "\n",
    "# training or inference\n",
    "best_gA_test = 0\n",
    "best_auc_test = 0\n",
    "skip_upto_epoch = 0\n",
    "skip_upto_batch = 0\n",
    "total_time = 0\n",
    "total_loss = 0\n",
    "total_accu = 0\n",
    "total_iter = 0\n",
    "total_samp = 0\n",
    "k = 0\n",
    "\n",
    "# Load model is specified\n",
    "if not (args.load_model == \"\"):\n",
    "    print(\"Loading saved model {}\".format(args.load_model))\n",
    "    if use_gpu:\n",
    "        if dlrm.ndevices > 1:\n",
    "            # NOTE: when targeting inference on multiple GPUs,\n",
    "            # load the model as is on CPU or GPU, with the move\n",
    "            # to multiple GPUs to be done in parallel_forward\n",
    "            ld_model = torch.load(args.load_model)\n",
    "        else:\n",
    "            # NOTE: when targeting inference on single GPU,\n",
    "            # note that the call to .to(device) has already happened\n",
    "            ld_model = torch.load(\n",
    "                args.load_model,\n",
    "                map_location=torch.device('cuda')\n",
    "                # map_location=lambda storage, loc: storage.cuda(0)\n",
    "            )\n",
    "    else:\n",
    "        # when targeting inference on CPU\n",
    "        ld_model = torch.load(args.load_model, map_location=torch.device('cpu'))\n",
    "    dlrm.load_state_dict(ld_model[\"state_dict\"])\n",
    "    ld_j = ld_model[\"iter\"]\n",
    "    ld_k = ld_model[\"epoch\"]\n",
    "    ld_nepochs = ld_model[\"nepochs\"]\n",
    "    ld_nbatches = ld_model[\"nbatches\"]\n",
    "    ld_nbatches_test = ld_model[\"nbatches_test\"]\n",
    "    ld_gA = ld_model[\"train_acc\"]\n",
    "    ld_gL = ld_model[\"train_loss\"]\n",
    "    ld_total_loss = ld_model[\"total_loss\"]\n",
    "    ld_total_accu = ld_model[\"total_accu\"]\n",
    "    ld_gA_test = ld_model[\"test_acc\"]\n",
    "    ld_gL_test = ld_model[\"test_loss\"]\n",
    "    if not args.inference_only:\n",
    "        optimizer.load_state_dict(ld_model[\"opt_state_dict\"])\n",
    "        best_gA_test = ld_gA_test\n",
    "        total_loss = ld_total_loss\n",
    "        total_accu = ld_total_accu\n",
    "        skip_upto_epoch = ld_k  # epochs\n",
    "        skip_upto_batch = ld_j  # batches\n",
    "    else:\n",
    "        args.print_freq = ld_nbatches\n",
    "        args.test_freq = 0\n",
    "\n",
    "    print(\n",
    "        \"Saved at: epoch = {:d}/{:d}, batch = {:d}/{:d}, ntbatch = {:d}\".format(\n",
    "            ld_k, ld_nepochs, ld_j, ld_nbatches, ld_nbatches_test\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Training state: loss = {:.6f}, accuracy = {:3.3f} %\".format(\n",
    "            ld_gL, ld_gA * 100\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Testing state: loss = {:.6f}, accuracy = {:3.3f} %\".format(\n",
    "            ld_gL_test, ld_gA_test * 100\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DLRM_Net(\n",
       "  (emb_l): ModuleList(\n",
       "    (0): EmbeddingBag(988, 16, mode=sum)\n",
       "    (1): EmbeddingBag(542, 16, mode=sum)\n",
       "    (2): EmbeddingBag(181190, 16, mode=sum)\n",
       "    (3): EmbeddingBag(79476, 16, mode=sum)\n",
       "    (4): EmbeddingBag(230, 16, mode=sum)\n",
       "    (5): EmbeddingBag(14, 16, mode=sum)\n",
       "    (6): EmbeddingBag(10417, 16, mode=sum)\n",
       "    (7): EmbeddingBag(443, 16, mode=sum)\n",
       "    (8): EmbeddingBag(3, 16, mode=sum)\n",
       "    (9): EmbeddingBag(23543, 16, mode=sum)\n",
       "    (10): EmbeddingBag(4640, 16, mode=sum)\n",
       "    (11): EmbeddingBag(163044, 16, mode=sum)\n",
       "    (12): EmbeddingBag(3116, 16, mode=sum)\n",
       "    (13): EmbeddingBag(26, 16, mode=sum)\n",
       "    (14): EmbeddingBag(8074, 16, mode=sum)\n",
       "    (15): EmbeddingBag(128285, 16, mode=sum)\n",
       "    (16): EmbeddingBag(10, 16, mode=sum)\n",
       "    (17): EmbeddingBag(3611, 16, mode=sum)\n",
       "    (18): EmbeddingBag(1692, 16, mode=sum)\n",
       "    (19): EmbeddingBag(4, 16, mode=sum)\n",
       "    (20): EmbeddingBag(148667, 16, mode=sum)\n",
       "    (21): EmbeddingBag(15, 16, mode=sum)\n",
       "    (22): EmbeddingBag(14, 16, mode=sum)\n",
       "    (23): EmbeddingBag(29185, 16, mode=sum)\n",
       "    (24): EmbeddingBag(65, 16, mode=sum)\n",
       "    (25): EmbeddingBag(21828, 16, mode=sum)\n",
       "  )\n",
       "  (bot_l): Sequential(\n",
       "    (0): Linear(in_features=13, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (top_l): Sequential(\n",
       "    (0): Linear(in_features=367, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_upto_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.nepochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time/loss/accuracy (if enabled):\n",
      "Finished training it 1024/3070 of epoch 0, 24.21 ms/it, loss 0.519843, accuracy 75.326 %\n",
      "Saving model to /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/dummy_model.pt\n",
      "Testing at - 1024/3070 of epoch 0, loss 0.515687, accuracy 75.489 %, best 75.489 %\n",
      "Finished training it 2048/3070 of epoch 0, 24.60 ms/it, loss 0.501983, accuracy 76.567 %\n",
      "Testing at - 2048/3070 of epoch 0, loss 0.513784, accuracy 75.351 %, best 75.489 %\n",
      "Finished training it 3070/3070 of epoch 0, 25.28 ms/it, loss 0.497440, accuracy 76.692 %\n",
      "Saving model to /Users/seuzha/Desktop/ml_data_set/criteo_lab/dac/dummy_model.pt\n",
      "Testing at - 3070/3070 of epoch 0, loss 0.508248, accuracy 76.051 %, best 76.051 %\n"
     ]
    }
   ],
   "source": [
    "print(\"time/loss/accuracy (if enabled):\")\n",
    "with torch.autograd.profiler.profile(args.enable_profiling, use_gpu) as prof:\n",
    "    while k < args.nepochs:\n",
    "        if k < skip_upto_epoch:\n",
    "            continue\n",
    "\n",
    "        accum_time_begin = time_wrap(use_gpu)\n",
    "\n",
    "        if args.mlperf_logging:\n",
    "            previous_iteration_time = None\n",
    "\n",
    "        for j, (X, lS_o, lS_i, T) in enumerate(train_ld):\n",
    "            if j < skip_upto_batch:\n",
    "                continue\n",
    "\n",
    "            if args.mlperf_logging:\n",
    "                current_time = time_wrap(use_gpu)\n",
    "                if previous_iteration_time:\n",
    "                    iteration_time = current_time - previous_iteration_time\n",
    "                else:\n",
    "                    iteration_time = 0\n",
    "                previous_iteration_time = current_time\n",
    "            else:\n",
    "                t1 = time_wrap(use_gpu)\n",
    "\n",
    "            # early exit if nbatches was set by the user and has been exceeded\n",
    "            if nbatches > 0 and j >= nbatches:\n",
    "                break\n",
    "            '''\n",
    "            # debug prints\n",
    "            print(\"input and targets\")\n",
    "            print(X.detach().cpu().numpy())\n",
    "            print([np.diff(S_o.detach().cpu().tolist()\n",
    "                   + list(lS_i[i].shape)).tolist() for i, S_o in enumerate(lS_o)])\n",
    "            print([S_i.detach().cpu().numpy().tolist() for S_i in lS_i])\n",
    "            print(T.detach().cpu().numpy())\n",
    "            '''\n",
    "\n",
    "            # forward pass\n",
    "            Z = dlrm_wrap(X, lS_o, lS_i, use_gpu, device)\n",
    "\n",
    "            # loss\n",
    "            E = loss_fn_wrap(Z, T, use_gpu, device)\n",
    "            '''\n",
    "            # debug prints\n",
    "            print(\"output and loss\")\n",
    "            print(Z.detach().cpu().numpy())\n",
    "            print(E.detach().cpu().numpy())\n",
    "            '''\n",
    "            # compute loss and accuracy\n",
    "            L = E.detach().cpu().numpy()  # numpy array\n",
    "            S = Z.detach().cpu().numpy()  # numpy array\n",
    "            T = T.detach().cpu().numpy()  # numpy array\n",
    "            mbs = T.shape[0]  # = args.mini_batch_size except maybe for last\n",
    "            A = np.sum((np.round(S, 0) == T).astype(np.uint8))\n",
    "\n",
    "            if not args.inference_only:\n",
    "                # scaled error gradient propagation\n",
    "                # (where we do not accumulate gradients across mini-batches)\n",
    "                optimizer.zero_grad()\n",
    "                # backward pass\n",
    "                E.backward()\n",
    "                # debug prints (check gradient norm)\n",
    "                # for l in mlp.layers:\n",
    "                #     if hasattr(l, 'weight'):\n",
    "                #          print(l.weight.grad.norm().item())\n",
    "\n",
    "                # optimizer\n",
    "                optimizer.step()\n",
    "\n",
    "            if args.mlperf_logging:\n",
    "                total_time += iteration_time\n",
    "            else:\n",
    "                t2 = time_wrap(use_gpu)\n",
    "                total_time += t2 - t1\n",
    "            total_accu += A\n",
    "            total_loss += L * mbs\n",
    "            total_iter += 1\n",
    "            total_samp += mbs\n",
    "\n",
    "            should_print = ((j + 1) % args.print_freq == 0) or (j + 1 == nbatches)\n",
    "            should_test = (\n",
    "                (args.test_freq > 0)\n",
    "                and (args.data_generation == \"dataset\")\n",
    "                and (((j + 1) % args.test_freq == 0) or (j + 1 == nbatches))\n",
    "            )\n",
    "\n",
    "            # print time, loss and accuracy\n",
    "            if should_print or should_test:\n",
    "                gT = 1000.0 * total_time / total_iter if args.print_time else -1\n",
    "                total_time = 0\n",
    "\n",
    "                gA = total_accu / total_samp\n",
    "                total_accu = 0\n",
    "\n",
    "                gL = total_loss / total_samp\n",
    "                total_loss = 0\n",
    "\n",
    "                str_run_type = \"inference\" if args.inference_only else \"training\"\n",
    "                print(\n",
    "                    \"Finished {} it {}/{} of epoch {}, {:.2f} ms/it, \".format(\n",
    "                        str_run_type, j + 1, nbatches, k, gT\n",
    "                    )\n",
    "                    + \"loss {:.6f}, accuracy {:3.3f} %\".format(gL, gA * 100)\n",
    "                )\n",
    "                # Uncomment the line below to print out the total time with overhead\n",
    "                # print(\"Accumulated time so far: {}\" \\\n",
    "                # .format(time_wrap(use_gpu) - accum_time_begin))\n",
    "                total_iter = 0\n",
    "                total_samp = 0\n",
    "\n",
    "            # testing\n",
    "            if should_test and not args.inference_only:\n",
    "                # don't measure training iter time in a test iteration\n",
    "                if args.mlperf_logging:\n",
    "                    previous_iteration_time = None\n",
    "\n",
    "                test_accu = 0\n",
    "                test_loss = 0\n",
    "                test_samp = 0\n",
    "\n",
    "                accum_test_time_begin = time_wrap(use_gpu)\n",
    "                if args.mlperf_logging:\n",
    "                    scores = []\n",
    "                    targets = []\n",
    "\n",
    "                for i, (X_test, lS_o_test, lS_i_test, T_test) in enumerate(test_ld):\n",
    "                    # early exit if nbatches was set by the user and was exceeded\n",
    "                    if nbatches > 0 and i >= nbatches:\n",
    "                        break\n",
    "\n",
    "                    t1_test = time_wrap(use_gpu)\n",
    "\n",
    "                    # forward pass\n",
    "                    Z_test = dlrm_wrap(\n",
    "                        X_test, lS_o_test, lS_i_test, use_gpu, device\n",
    "                    )\n",
    "                    if args.mlperf_logging:\n",
    "                        S_test = Z_test.detach().cpu().numpy()  # numpy array\n",
    "                        T_test = T_test.detach().cpu().numpy()  # numpy array\n",
    "                        scores.append(S_test)\n",
    "                        targets.append(T_test)\n",
    "                    else:\n",
    "                        # loss\n",
    "                        E_test = loss_fn_wrap(Z_test, T_test, use_gpu, device)\n",
    "\n",
    "                        # compute loss and accuracy\n",
    "                        L_test = E_test.detach().cpu().numpy()  # numpy array\n",
    "                        S_test = Z_test.detach().cpu().numpy()  # numpy array\n",
    "                        T_test = T_test.detach().cpu().numpy()  # numpy array\n",
    "                        mbs_test = T_test.shape[0]  # = mini_batch_size except last\n",
    "                        A_test = np.sum((np.round(S_test, 0) == T_test).astype(np.uint8))\n",
    "                        test_accu += A_test\n",
    "                        test_loss += L_test * mbs_test\n",
    "                        test_samp += mbs_test\n",
    "\n",
    "                    t2_test = time_wrap(use_gpu)\n",
    "\n",
    "                if args.mlperf_logging:\n",
    "                    scores = np.concatenate(scores, axis=0)\n",
    "                    targets = np.concatenate(targets, axis=0)\n",
    "\n",
    "                    metrics = {\n",
    "                        'loss' : sklearn.metrics.log_loss,\n",
    "                        'recall' : lambda y_true, y_score:\n",
    "                        sklearn.metrics.recall_score(\n",
    "                            y_true=y_true,\n",
    "                            y_pred=np.round(y_score)\n",
    "                        ),\n",
    "                        'precision' : lambda y_true, y_score:\n",
    "                        sklearn.metrics.precision_score(\n",
    "                            y_true=y_true,\n",
    "                            y_pred=np.round(y_score)\n",
    "                        ),\n",
    "                        'f1' : lambda y_true, y_score:\n",
    "                        sklearn.metrics.f1_score(\n",
    "                            y_true=y_true,\n",
    "                            y_pred=np.round(y_score)\n",
    "                        ),\n",
    "                        'ap' : sklearn.metrics.average_precision_score,\n",
    "                        'roc_auc' : sklearn.metrics.roc_auc_score,\n",
    "                        'accuracy' : lambda y_true, y_score:\n",
    "                        sklearn.metrics.accuracy_score(\n",
    "                            y_true=y_true,\n",
    "                            y_pred=np.round(y_score)\n",
    "                        ),\n",
    "                        # 'pre_curve' : sklearn.metrics.precision_recall_curve,\n",
    "                        # 'roc_curve' :  sklearn.metrics.roc_curve,\n",
    "                    }\n",
    "\n",
    "                    # print(\"Compute time for validation metric : \", end=\"\")\n",
    "                    # first_it = True\n",
    "                    validation_results = {}\n",
    "                    for metric_name, metric_function in metrics.items():\n",
    "                        # if first_it:\n",
    "                        #     first_it = False\n",
    "                        # else:\n",
    "                        #     print(\", \", end=\"\")\n",
    "                        # metric_compute_start = time_wrap(False)\n",
    "                        validation_results[metric_name] = metric_function(\n",
    "                            targets,\n",
    "                            scores\n",
    "                        )\n",
    "                        # metric_compute_end = time_wrap(False)\n",
    "                        # met_time = metric_compute_end - metric_compute_start\n",
    "                        # print(\"{} {:.4f}\".format(metric_name, 1000 * (met_time)),\n",
    "                        #      end=\"\")\n",
    "                    # print(\" ms\")\n",
    "                    gA_test = validation_results['accuracy']\n",
    "                    gL_test = validation_results['loss']\n",
    "                else:\n",
    "                    gA_test = test_accu / test_samp\n",
    "                    gL_test = test_loss / test_samp\n",
    "\n",
    "                is_best = gA_test > best_gA_test\n",
    "                if is_best:\n",
    "                    best_gA_test = gA_test\n",
    "                    if not (args.save_model == \"\"):\n",
    "                        print(\"Saving model to {}\".format(args.save_model))\n",
    "                        torch.save(\n",
    "                            {\n",
    "                                \"epoch\": k,\n",
    "                                \"nepochs\": args.nepochs,\n",
    "                                \"nbatches\": nbatches,\n",
    "                                \"nbatches_test\": nbatches_test,\n",
    "                                \"iter\": j + 1,\n",
    "                                \"state_dict\": dlrm.state_dict(),\n",
    "                                \"train_acc\": gA,\n",
    "                                \"train_loss\": gL,\n",
    "                                \"test_acc\": gA_test,\n",
    "                                \"test_loss\": gL_test,\n",
    "                                \"total_loss\": total_loss,\n",
    "                                \"total_accu\": total_accu,\n",
    "                                \"opt_state_dict\": optimizer.state_dict(),\n",
    "                            },\n",
    "                            args.save_model,\n",
    "                        )\n",
    "\n",
    "                if args.mlperf_logging:\n",
    "                    is_best = validation_results['roc_auc'] > best_auc_test\n",
    "                    if is_best:\n",
    "                        best_auc_test = validation_results['roc_auc']\n",
    "\n",
    "                    print(\n",
    "                        \"Testing at - {}/{} of epoch {},\".format(j + 1, nbatches, k)\n",
    "                        + \" loss {:.6f}, recall {:.4f}, precision {:.4f},\".format(\n",
    "                            validation_results['loss'],\n",
    "                            validation_results['recall'],\n",
    "                            validation_results['precision']\n",
    "                        )\n",
    "                        + \" f1 {:.4f}, ap {:.4f},\".format(\n",
    "                            validation_results['f1'],\n",
    "                            validation_results['ap'],\n",
    "                        )\n",
    "                        + \" auc {:.4f}, best auc {:.4f},\".format(\n",
    "                            validation_results['roc_auc'],\n",
    "                            best_auc_test\n",
    "                        )\n",
    "                        + \" accuracy {:3.3f} %, best accuracy {:3.3f} %\".format(\n",
    "                            validation_results['accuracy'] * 100,\n",
    "                            best_gA_test * 100\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        \"Testing at - {}/{} of epoch {},\".format(j + 1, nbatches, 0)\n",
    "                        + \" loss {:.6f}, accuracy {:3.3f} %, best {:3.3f} %\".format(\n",
    "                            gL_test, gA_test * 100, best_gA_test * 100\n",
    "                        )\n",
    "                    )\n",
    "                # Uncomment the line below to print out the total time with overhead\n",
    "                # print(\"Total test time for this group: {}\" \\\n",
    "                # .format(time_wrap(use_gpu) - accum_test_time_begin))\n",
    "\n",
    "                if (args.mlperf_logging\n",
    "                    and (args.mlperf_acc_threshold > 0)\n",
    "                    and (best_gA_test > args.mlperf_acc_threshold)):\n",
    "                    print(\"MLPerf testing accuracy threshold \"\n",
    "                          + str(args.mlperf_acc_threshold)\n",
    "                          + \" reached, stop training\")\n",
    "                    break\n",
    "\n",
    "                if (args.mlperf_logging\n",
    "                    and (args.mlperf_auc_threshold > 0)\n",
    "                    and (best_auc_test > args.mlperf_auc_threshold)):\n",
    "                    print(\"MLPerf testing auc threshold \"\n",
    "                          + str(args.mlperf_auc_threshold)\n",
    "                          + \" reached, stop training\")\n",
    "                    break\n",
    "\n",
    "        k += 1  # nepochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
